---
title: "Analysis of metabolomics data"
author: "Jessica Cooperstone"
format: 
  revealjs:
    slide-number: true
    logo: "img/noun-metabolomics.png"
    footer: "© Jessica Cooperstone, 2024"
    theme: simple
    echo: true
    incremental: true
---

```{r include = FALSE}
library(tidyverse)
library(glue)
```

## Now that we have our feature table, what happens next?

-   Data normalization/transformation/scaling
-   Assess for quality (and make adjustments if needed)
-   Univariate analysis (supervised)
-   Multivariate analysis (supervised or unsupervised)

## Data pre-treatment

![Different types of data centering, scaling, and transformation, from [van den Berg et al., Genomics 2006](https://doi.org/10.1186/1471-2164-7-142)](img/04/center-scaling-transformation.png)


## Centering 

::: {style="font-size: 23px"}
-   Centering converts all relative intensities to be centered around zero instead of around mean concentrations
-   This is particularly useful in metabolomics given that more *intense* features are *not necessarily more abundant*
-   I like to center when conducting principal components analysis (PCA, more on this later)

::: columns
::: {.column width="50%" style="font-size: 23px" .fragment}
![PCA not centered](img/04/pca_not_centered.png){width="80%"}
:::

::: {.column width="50%" style="font-size: 23px" .fragment}
![PCA centered](img/04/pca_centered.png){width="80%"}
:::
:::
:::

## Scaling

-   Uses data dispersion as a part of a scaling factor
    -   Auto scaling: mean = 0, standard deviation = 1 (for each feature)
    -   Pareto scaling: like auto but scaled to square root of the st dev. The most common scaling used for metabolomics
    -   In general I find this not necessary and will just log transform (more coming next)

## Log transformation - tables

-   Reduce heteroscedasicity
-   Makes large values proportionally smaller than small ones
-   Makes distributions more normal

```{r include = FALSE}
transformations <- tribble(
  ~sample, ~feature_1, ~feature_2,
  "control", 10000, 1000,
  "treatment", 11000, 2000
)

log2_transformations <- tribble(
  ~sample, ~log2_feature_1, ~log2_feature_2,
  "control", log2(10000), log2(1000),
  "treatment", log2(11000), log2(2000)
)
```

::: columns
::: {.column width="50%" style="font-size: 23px" .fragment}
```{r}
#| echo: false
knitr::kable(transformations)
```
:::

::: {.column width="50%" style="font-size: 23px" .fragment}
```{r}
#| echo: false
knitr::kable(log2_transformations, digits = 2)
```
:::
:::

## Log transformation - plots

-   Log transforming can make your data look more normally distributed

```{r include = FALSE}
raw <- as.data.frame(rlnorm(10000,meanlog = 2,sdlog = 0.5)) %>% 
  rename(value = 1)

log2 <- raw %>%
  mutate(value = log2(value))
```

::: columns
::: {.column width="50%" style="font-size: 25px" .fragment}
```{r}
#| echo: false

raw %>%
  ggplot(aes(x = value)) +
  geom_histogram(color = "black") +
  labs(title = "A left skewed distribution")
```
:::

::: {.column width="50%" style="font-size: 25px" .fragment}
```{r}
#| echo: false
#| 
log2 %>%
  ggplot(aes(x = value)) +
  geom_histogram(color = "black") +
  labs(title = "Same distribution log2 normalized")
```
:::
:::

## Sample wise normalization

-   Adjust based on weight or volume (very common)
-   Adjust based on intensity of an internal standard (used sometimes)
-   Adjust based on total signal (used in older MS papers and for NMR but not really in MS)


## Assessing data quality

-   How can we go about assessing if our data is of high quality when we have thousands of features and don't know what to expect?

## Overlay your BPCs

![Overlaid base peak chromatograms in MZmine](img/04/qc-bpc.png)

## Zoom in and look at your BPCs carefully

![Overlaid base peak chromatograms in MassHunter](img/04/qc-bpc-zoom.png)

## Look at a single feature for retention time shifting

![Overlaid extracted ion chromatograms in MassHunter](img/04/qc-eic-zoom.png)

## Create boxplots of features by sample

::: columns
::: {.column width="50%" style="font-size: 25px"}
Ordering by sample groups ![](img/04/quality-boxplot-log2.png)
:::

::: {.column width="50%" style="font-size: 25px"}
Ordering by run order ![](img/04/quality-boxplot-log2-run-order.png)
:::
:::

## Your QCs should cluster in a PCA

![A PCA where the QCs cluster beautifully](img/04/pca_centered.png)

## Data analysis can be unsupervised or supervised

::: columns
::: {.column width="50%" style="font-size: 25px"}
### Unsupervised (model does not know which samples belong to which groups):

-   Principal components analysis (PCA)
-   Hierarchical clustering analysis (HCA)
-   K-means clustering
:::

::: {.column width="50%" style="font-size: 25px"}
### Supervised (model knows group composition):

-   Univariate (or multivariate) group comparisons
-   Predictive/classification models
    -   Partial least squares regression (PLS-R)
    -   PLS-discriminant analysis (PLS-DA)
    -   Random forest
    -   Artificial neural networks
    -   Others
:::
:::

## Unsupervised: Principal Components Analysis (PCA)

::: columns
::: {.column width="50%" style="font-size: 25px"}
-   A dimensionality reduction approach that transforms our data into a new system where principal coordinates (PCs) are drawn maximizing variation in our data.
-   Each new PC is orthogonal to the previous one.
-   Can interpret points closer together as more similar that those further apart
-   The loadings plot helps us understand which features are most influential for each PC
:::

::: {.column width="50%" style="font-size: 25px" .fragment}

![Fig from [https://www.ibm.com/topics/principal-component-analysis](https://www.ibm.com/topics/principal-component-analysis)](img/04/pca-explained.png)

:::
:::

## Interpreting PCAs

![PCA A) scores and B) loadings (from [Dzakovich et al., 2022](https://doi.org/10.1002/tpg2.20192))](img/04/pca-interpretation.png)


## What you should be doing with your PCAs

-   Check for samples that are very different from the rest
-   Show that your QCs cluster together
-   Look for batch effects
-   Observe the distance between your samples within a group, and between groups
-   See how much variation is being explained by each PC
-   Look for clusters in your data

## Unsupervised: Hierarchical Clustering Analysis (HCA)

::: {style="font-size: 30px"}
-   An approach to "cluster" samples based on their distance/similarity
-   Find two most similar samples and merge into cluster, and repeat
-   Different methods for determining both distance and linkage
-   Like a phylogenic tree - samples that are closer together are more similar
-   Can cluster samples, features, or both
-   [StatQuest video on hierarchical clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo)

:::

## HCA applied to metabolomics

![A hierarchical clustering example from [Dzakovich et al., 2024](https://doi.org/10.1002/mnfr.202300239)](img/04/hca-example.jpeg)


## Unsupervised: K-means clustering

::: columns
::: {.column width="50%" style="font-size: 30px" .incremental}
-   Randomly assigns all samples to be a part of $k$ clusters
-   Calculates centroids and assigns samples to the nearest one
-   Iterates until centroids stability or reach some maximum
-   Can figure out number of clusters using a scree plot to visualize within cluster sums
-   [StatQuest explains K-means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA)
:::

::: {.column width="50%" style="font-size: 25px" .fragment}
![From [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=k%2Dmeans%20clustering%20is%20a,a%20prototype%20of%20the%20cluster.)](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)

:::
:::

## Supervised: Univariate testing, the categories

-   Two group tests
    -   t-test (parametric)
    -   Wilcoxon rank sum test (non-parameric)
-   More than two group tests
    -   ANOVA (parametric)
    -   Kruskal Wallis test (non-parametric)
    -   Both followed by post-hoc means separation

## Big p, little n: the curse of high dimensionality 

::: {style="font-size: 30px"}

-   With metabolomics, we almost always have more features (p) than samples (n)
-   This means we are making a lot of comparisons
-   If we want to know if each figure is differentially abundant across two groups, we can do a series of t-tests.
-   If we set α = 0.05, then we have a 5% chance for rejecting the null when it is true (false positive, type I error)
-   If we do 1000 tests, and each one has a 5% of a false positive, then we would predict to have 50 false positives due to chance alone - features that have p \< 0.05 but are not truly different between our groups

:::

## Ways to control for multiple testing

-   Bonferroni correction: $α/number\,of\,comparisons$ (very conservative, leads to false negatives)
-   Benjamini Hochberg false discovery rate correction: adjust overall error rate to be α (tries to balance false negatives and positives)

::: {style="font-size: 20px" .fragment}
![Fig. from [https://geneviatechnologies.com/blog/what-is-multiple-testing-correction/](https://geneviatechnologies.com/blog/what-is-multiple-testing-correction/)](img/04/fdr.png){fig-align="center" width="800px"}
:::

## Should I use parametric or non-parametric tests?

## Multivariate testing in metabolomics

-   Multivariate approaches allow the investigation of more than one feature at once
-   Approaches can be unsupervised or supervised
-   Be sure to select the most appropriate method given the nature of your data and question

## Supervised: PLS-DA

::: columns
::: {.column width="50%" style="font-size: 30px"}
-   Partial least squares discriminate analysis (PLS-DA) approaches optimize separation between groups
-   Two data matrices, X: contains your features, Y: contains group identity
-   Be sure to look at R^2 and Q^2 for both training and test sets
-   Has a tendency to overfit
-   Learn more about PLS-DA: [Brereton and Lloyd, J Chemometrics 2014](https://doi.org/10.1002/cem.2609), [Ruiz-Perez et al., BMC Bioinformatics 2020](https://doi.org/10.1186/s12859-019-3310-7), [Worley and Powers, Curr Metabolomics 2013](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4465187/).
:::

::: {.column width="50%" style="font-size: 25px" .fragment}
![Ruiz-Perez et al., BMC Bioinformatics 2020](img/04/pca-vs-plsda.png)


:::
:::


## Random forest

::: columns
::: {.column width="40%" style="font-size: 30px"}
-   A machine learning method for **classification** via construction of decision trees
-   Trees are created with a random subset of variables
-   Increase to 100% purity at the base of the tree
-   Do this over and over
-   A set (~1/3rd) is left out of the sample for each tree - this is the "out of bag (oob)" data, and can be used to see how accurate your classification is
-   Learn more by watching this [StatQuest video](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)
:::

::: {.column width="60%" style="font-size: 25px" .fragment}
![An example of a simple, single decision tree](img/04/decision-tree.svg)

:::
:::

## Artificial neural networks

::: columns
::: {.column width="40%" style="font-size: 30px"}
-   Conceptually like a network of connected neurons, where each connection is weighted and a function transforms the sums to form the output
-   Input data is mapped to latent structures, which are then mapped to output data
-   Sometimes can be a "black box"
-   Learn more by watching this [StatQuest video](https://www.youtube.com/watch?v=CqOfi41LfDw)
:::

::: {.column width="60%" style="font-size: 25px" .fragment}
![An illustration of a single neural network, from [Mendez et al., Metabolomics 2019]( https://doi.org/10.1007/s11306-019-1608-0)](img/04/ann.svg)

:::
:::

## Others