---
title: "Data analysis with MetaboAnalyst"
author: "Jessica Cooperstone"
title-block-banner: false
number-depth: 4
number-sections: true
editor_options:
  chunk_output_type: console
knitr:
  opts_chunk:
    out.width: "85%"
    class-output: styled-output
bibliography: references.bib
---

# Introduction

Now that we have a filtered, final feature table, we can get started on our data analysis, including understanding if our data is of good quality before we begin to make biological interpretations as to what our results mean. We are going to conduct this analysis using [MetaboAnalyst](https://www.metaboanalyst.ca/) which is at version 6.0 when I put together this tutorial. You can read more about MetaboAnalyist on their website or in their publications [@ewald2024].

## Data formatting

Before we upload our data, let's make sure its in a suitable format. If we go to the [Data Formats link](https://www.metaboanalyst.ca/docs/Format.xhtml) on the left side of the website, we can see example data sets which will help us format our data. Our data looks like this right out of MZmine:

![Our data, straight from MZmine](img/04/raw.png)

> Our data contains samples in rows, and features in columns.

We will start with doing statistics (one factor) and we have a peak intensity table, so the file called [`lcms_table.csv`](https://www.xialab.ca/api/download/metaboanalyst/lcms_table.csv) is a good one for us to look at.

![LCMS sample data](img/04/lc-ms-table.png)

In this data, we have features in rows and samples in columns. There is also a header row which contains the unique sample names, and a second row called Label which contains the groups. We will need to adjust our data to look like this. This includes

-   Adding a row called Label which contains the sample groups (here, QC, HATS, LA2213, or OH8243).

![Add a row which contains Label and the tomato identities](img/04/add-label.png)

-   Creating a single column with the mz_rt as a unique identifier. We can do this with the function `=CONCATENATE()` in Excel. Then we can fill down the row. Then be sure to copy this column and `Paste` \> `Paste special` \> `Values` since when we later remove some of the other columns that are inputs to this one, the formulas will break.

![How to combine the row m/z and row retention time columns to be one unique mz_rt](img/04/add-mz_rt.png)

-   Now we can remove row ID, row m/z and row retention time.

Our data should now look like this:

![Our data ready to import into MetaboAnalyst](img/04/data-for-metaboanalyst.png)

# Import into MetaboAnalyst

Remember that when you are using MetaboAnalyst through the web browser that it will eventually time out, so starting an analysis, and walking away for a period of time is not a good idea. Make sure you keep good notes on what you do during your analysis.

Navigate to <https://www.metaboanalyst.ca/> and click on the red "Click here to start button".

![Get started in MetaboAnalyst](img/04/metaboanalyst-start.png)

We have already processed out spectra so we are going to click on Statistical Analysis (one factor).

![Begin statistical analysis in MetaboAnalyst](img/04/statistical-analysis.png)

Now we want to upload our data. We have our file saved as a .csv and our data is as peak intensities. In our case, samples are in columns and data is unpaired. Once we have done this, we can press submit just to the right of our input.

![Upload our data](img/04/upload.png)

# Data processing

## Data check

Once the data has been imported, we can first look at the data integrity check.

![Data integrity check](img/04/integrity-check.png)

We see that:

-   Samples are in columns and features in rows
-   Our data is in `.csv` format
-   We have 42 samples and 2514 peaks - this is what we would expect
-   Samples are not paired - this is what we would expect
-   3 groups were dtected - I might expect 4 groups - OH8243, LA2213, HATS, and QCs. If we click the "Edit Groups" button we can see that the Label for each of our groups has inherited correctly, but that it seems like QC isn't being counted as a group.
-   A total of 0 (0%) missing values were detected - this makes sense as data that is missing here is coded as zero.

If we agree with this, we can click Proceed. MetaboAnalyst will bring you directly to the Data filtering step, but we can go back one to see how missing values are handled.

## Missing values

Here you can tell MetaboAnalyst what you want done with missing values. Here we don't have any, but if you did you could indicate:

-   Whether you want to remove features that have a lot of missing values
-   What you want to do to estimate the remaining missing values.

## Data filters

In this step, we can filter variables based on different rules. These include:

-   Features that are very variable i nthe QCs. We don't need to do this because we already did it manually.
-   Features that are near constant across the conditions (i.e., those that have very low standard deviations)
-   Features that are very low

I prefer to do this kind of filtering outside of MetaboAnalyst - you could do this in Excel if you wanted to. Here, we are not going to do any more filtering. We can just click Proceed (and skip this filtering).

# Normalization

We can normalize our samples in different ways. MetaboAnalyst gives is 3 categories of transformation types:

-   Sample normalization for general adjustment for systemic differences across samples. Here would could adjust by weight, by a reference features, or by total signal.
-   Data transformation including by log (base 10), square root, or cubed room
-   Data scaling

You can pick a nornalization and view the result. This is nice because you can see what your normalization will do before picking the most appropriate one.

For example, I am going to try just log10 transforming my samples. When I do this, I can look at the result from both a feature and sample view.

::: panel-tabset
## Feature view
![Feature view after log10 normalization](img/04/log10-feature-view.png)

## Sample view
![Sample view after log10 normalization](img/04/log10-sample-view.png)
:::

I can contrast this with a log10 transformation and then Pareto scaling.

::: panel-tabset
## Feature view
![Feature view after log10 normalization](img/04/log10-pareto-feature-view.png)

## Sample view
![Sample view after log10 normalization](img/04/log10-pareto-sample-view.png)
:::

I think these look pretty similar and I think it would be ok to use either transformation. I am going to continue the rest of the analysis with just the log10 transformed data.

# Statistics

Now that our data is set, we can use different analysis methods to understand our data quality, and differences among sample groups. Remember, there are analyses we want to do both with and without the QCs. I like to see a PCA that includes the QCs, but then I'd remove them and retain our three tomato groups for further analysis.

Let's look with our QCs at the PCA first, and then we will remove those samples.

## PCA with our QCs

When we look at the PCA tab, we can see:

-   An overview - which includes PCA scores plot for each combination of PCs 1 through 5. The most conventional one to look at would be PC1 vs PC2.
-   A scree plot - this shows us how much variation is explained by each of the PCs
-   A 2D scores plot - this shows us each sample's new coordinates in a reduced dimensionality space. We can indicate what PC should be on which axis (though again it's hard for me to think of a reason to do something different from PC1 on x and PC2 on y)
-   A loadings plot - this shows us how the variables (here, features) are weighted in contributing to each PC
-   3D plots - I find these to be terrible and never use them
-   A biplot - combines the scores and the loadings, and can get very overplotted for MS metabolomics data.

Let's look at each plot, talk about what it shows us, and try to interpret the data in the context of this particular experiment.

![](img/04/pca-qcs-scores-5PC.png)    

In the PCA overview, we can see a big chunk of variation (63%) is explained on PC1. We also see that our groups are really clearly separated when viewing PC1 vs. PC2.

![](img/04/pca-qc-scree.png)

When looking at the scree plot, we again see a large percentage of variation explained by PC1, and less variation explained by each subsequent PC. This is going to always be the case - PC2 always explains more variation than PC3, and PC3 more than PC4. But we can interpret this to be that drawing two PCs explains 78.4% of the metabolic variation in our dataset, and that adding additional PCs doesn't explain all that much additional variation.

![](img/04/pca-qc-scores.png)

When we think of PCAs, what we are typically thinking of is a scores plot. In this one, we see a few important things:

-   Our QCs are clustering very closely together - this is a good piece of data to convince both ourselves and our readers that our data is of high quality. We are able to measure the same thing very reproducibly.
-   PC1 mostly separates LA2213 (the wild tomato) from OH8243 and HATS (the cultivated tomatoes). 
-   PC2 mostly separates OH8243 (the commercial parent) from HATS (the tomato that has been introgressed for the trait of high steroidal alkaloids).
-   Overall, HATS and OH8243 are more similar than either of those two are to LA2213.

You can adjust in MetaboAnalyst how this plot looks. I tend to turn sample names and confidence intervals off because I think they crowd the plot. You can click on the little paint palette to be able to specify how your plot looks (change colors and shapes) and download a high quality image.

I would always put a figure like this in the supplementary materials of my papers so readers can get confidence that our data is worth spending time interpreting.

![](img/04/pca-qc-loadings.png)

The loadings plot helps us to interpret that is driving separation on PC1 and PC2. Interpreting this alongside our scores plot, in this case, points on the far right of the plot should be higher in LA2213 and points on the far left should be higher oh OH8243 and HATS. Points that are close to the top of the plot should be higher in HATS and those towards the bottom higher in OH8243 or LA2213.

For example, if we click on the right most point on the loadings plot, we can see that this represents the feature with the identifier 578.5296_5.0895. We can select a boxplot and see the relative intensity of this feature across our sample groups. 

![](img/04/578.5296215_5.0895395_qc.png)

Similarly, the point with the highest value on the y axis 474.3578_6.5878 is highest in HATS and lower in the other tomatoes.

![](img/04/474.3577933_6.587835_qc.png)

## PCA without QCs

That that we can see that our QCs are tightly clustering together, we can save that PCA scores plot and remove our QC samples. We can do that by navigating to `Processing` > `Data editor` in the left part of the browser.

![](img/04/data-editor.png)

Don't forget to re-normalize.

This dataset (without the QCs) is what I would use for the rest of this analysis.

## ANOVA

## Dendrogram (i.e., hierarchical clustering)

## K-means (a different type of clustering)

## Correlations

## Random forest


## Pattern hunter

# References