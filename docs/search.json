[
  {
    "objectID": "lectures/02_design-collection.html#the-idea-behind-design-and-sampling",
    "href": "lectures/02_design-collection.html#the-idea-behind-design-and-sampling",
    "title": "Study design and sample collection",
    "section": "The idea behind design and sampling",
    "text": "The idea behind design and sampling\n\n\nThe samples you choose should be representative of the broader population you wish to study\nThe variability between your groups should be due to your research question (and not some other thing)"
  },
  {
    "objectID": "lectures/02_design-collection.html#types-of-replicates",
    "href": "lectures/02_design-collection.html#types-of-replicates",
    "title": "Study design and sample collection",
    "section": "Types of replicates",
    "text": "Types of replicates"
  },
  {
    "objectID": "lectures/02_design-collection.html#sources-of-variation-in-samples",
    "href": "lectures/02_design-collection.html#sources-of-variation-in-samples",
    "title": "Study design and sample collection",
    "section": "Sources of variation in samples",
    "text": "Sources of variation in samples\n\n\nExperimental (due to our question):\n\nNon-experimental (due to things other than our question)\n\nPersonnel\nCollection materials\nStorage (e.g., freeze-thaw, age)\nInstrument issues\nRun order effects\n\n\n\n\nIdeally, measurement variability &lt;&lt;&lt; sample variability"
  },
  {
    "objectID": "lectures/02_design-collection.html#what-can-we-do-to-minimize-non-experimental-variation",
    "href": "lectures/02_design-collection.html#what-can-we-do-to-minimize-non-experimental-variation",
    "title": "Study design and sample collection",
    "section": "What can we do to minimize non-experimental variation?",
    "text": "What can we do to minimize non-experimental variation?\n\n\nConsistency!\nChoose appropriate controls\nEnsure sample groups are handled in the same way\nRandomize extraction order\nRandomize run order\nRun in one batch"
  },
  {
    "objectID": "lectures/02_design-collection.html#quality-control-qc-samples",
    "href": "lectures/02_design-collection.html#quality-control-qc-samples",
    "title": "Study design and sample collection",
    "section": "Quality control (QC) samples",
    "text": "Quality control (QC) samples\n\n\nHow do we know if our data looks ‚Äòokay‚Äô if we don‚Äôt know what it‚Äôs supposed to look like?\nPooled QCs, commercial QCs, synthetic QCs\n\n\n\n\n\n\n\n\nFig. adapted from Broadhurst et al., Metabolomics 2018"
  },
  {
    "objectID": "lectures/02_design-collection.html#processextraction-blanks",
    "href": "lectures/02_design-collection.html#processextraction-blanks",
    "title": "Study design and sample collection",
    "section": "Process/extraction blanks",
    "text": "Process/extraction blanks\n\n\nTo remove signal that comes from the non-sample parts of your extract, we run process blanks\nThese consist of all components of your extraction minus the sample\n\nIf you are extracting tomato juice with methanol, you would instead take an equal volume of water and ‚Äúextract‚Äù using the identical process\n\nThis allows the removal of signal coming from: tips, vials, extraction aids, solvents, tubing, your dirty MS source etc"
  },
  {
    "objectID": "lectures/02_design-collection.html#run-order",
    "href": "lectures/02_design-collection.html#run-order",
    "title": "Study design and sample collection",
    "section": "Run order",
    "text": "Run order\n\nFig. from John-Williams et al., Sci Data 2017"
  },
  {
    "objectID": "lectures/02_design-collection.html#qc-samples-should-cluster-together",
    "href": "lectures/02_design-collection.html#qc-samples-should-cluster-together",
    "title": "Study design and sample collection",
    "section": "QC samples should cluster together",
    "text": "QC samples should cluster together\n\nFig. adapted from Broadhurst et al., Metabolomics 2018"
  },
  {
    "objectID": "lectures/02_design-collection.html#qc-adjustment",
    "href": "lectures/02_design-collection.html#qc-adjustment",
    "title": "Study design and sample collection",
    "section": "QC adjustment",
    "text": "QC adjustment\n\nFig. from Dunn et al., Nature Protocols 2011"
  },
  {
    "objectID": "lectures/02_design-collection.html#sample-age",
    "href": "lectures/02_design-collection.html#sample-age",
    "title": "Study design and sample collection",
    "section": "Sample age",
    "text": "Sample age\n\nFig. from Kjeldahl and Bro, J Chemometrics 2010"
  },
  {
    "objectID": "lectures/02_design-collection.html#timing-of-collection",
    "href": "lectures/02_design-collection.html#timing-of-collection",
    "title": "Study design and sample collection",
    "section": "Timing of collection",
    "text": "Timing of collection\n\nFig. from Sato et al., Molecular Metabolism 2018"
  },
  {
    "objectID": "lectures/02_design-collection.html#collection-tube-impact",
    "href": "lectures/02_design-collection.html#collection-tube-impact",
    "title": "Study design and sample collection",
    "section": "Collection tube impact",
    "text": "Collection tube impact\n\nFig. from Cruickshank-Quinn et al., Metabolites 2018"
  },
  {
    "objectID": "lectures/02_design-collection.html#collection-variables-to-consider",
    "href": "lectures/02_design-collection.html#collection-variables-to-consider",
    "title": "Study design and sample collection",
    "section": "Collection variables to consider",
    "text": "Collection variables to consider\n\n\nTubes\nStorage\nFreeze-thaw\nQuenching"
  },
  {
    "objectID": "lectures/02_design-collection.html#standarizing-extraction-amoung",
    "href": "lectures/02_design-collection.html#standarizing-extraction-amoung",
    "title": "Study design and sample collection",
    "section": "Standarizing extraction amoung",
    "text": "Standarizing extraction amoung\nCompare samples by:\n\n\nMass\nVolume\nNumber of cells\nNormalized to protein\nSome other thing?"
  },
  {
    "objectID": "lectures/02_design-collection.html#when-starting-a-metabolomics-study",
    "href": "lectures/02_design-collection.html#when-starting-a-metabolomics-study",
    "title": "Study design and sample collection",
    "section": "When starting a metabolomics study‚Ä¶",
    "text": "When starting a metabolomics study‚Ä¶\n\n\nPurchase one lot of materials (solvents, reagents, vials etc.)\nStore many, small volume aliquots to avoid freeze-thaw\nDecide what your QCs will be and act accordingly\nBe consistent ‚Äì generate a plan!\nBe especially careful when conducting a secondary analysis of already collected data\n\n\n\n\n\n¬© Jessica Cooperstone, 2024"
  },
  {
    "objectID": "lectures/01_intro.html#introductions",
    "href": "lectures/01_intro.html#introductions",
    "title": "Introduction to metabolomics",
    "section": "Introductions üëã",
    "text": "Introductions üëã\n\n\nName\nResearch focus area\nHow you see yourself using metabolomics\nWhat you hope to learn"
  },
  {
    "objectID": "lectures/01_intro.html#what-is-metabolomics",
    "href": "lectures/01_intro.html#what-is-metabolomics",
    "title": "Introduction to metabolomics",
    "section": "What is metabolomics?",
    "text": "What is metabolomics?\n\n\nThe study of the totality of small molecules (&lt; 1500 Da) within a given system\nMetabolomics is a tool that allows us to study global metabolism"
  },
  {
    "objectID": "lectures/01_intro.html#metabolites-are-the-downstream-products-of-the-system-biology-cascade",
    "href": "lectures/01_intro.html#metabolites-are-the-downstream-products-of-the-system-biology-cascade",
    "title": "Introduction to metabolomics",
    "section": "Metabolites are the downstream products of the system biology cascade",
    "text": "Metabolites are the downstream products of the system biology cascade"
  },
  {
    "objectID": "lectures/01_intro.html#the-food-metabolome-can-be-influenced-by",
    "href": "lectures/01_intro.html#the-food-metabolome-can-be-influenced-by",
    "title": "Introduction to metabolomics",
    "section": "The food metabolome can be influenced by:",
    "text": "The food metabolome can be influenced by:\n\n\nVariety/genetics\nEnvironment\nPost-harvest/processing\nStorage"
  },
  {
    "objectID": "lectures/01_intro.html#the-metabolome-is-really-big",
    "href": "lectures/01_intro.html#the-metabolome-is-really-big",
    "title": "Introduction to metabolomics",
    "section": "The metabolome is really BIG!",
    "text": "The metabolome is really BIG!\n\n\n\nAdapted from David Wishart‚Äôs Canadian Bioinformatics Workshop"
  },
  {
    "objectID": "lectures/01_intro.html#the-metabolome-is-very-chemically-diverse",
    "href": "lectures/01_intro.html#the-metabolome-is-very-chemically-diverse",
    "title": "Introduction to metabolomics",
    "section": "The metabolome is very chemically diverse",
    "text": "The metabolome is very chemically diverse\n\n\nFrom Roche, Biochemical Pathways"
  },
  {
    "objectID": "lectures/01_intro.html#the-metabolome-is-constantly-changing",
    "href": "lectures/01_intro.html#the-metabolome-is-constantly-changing",
    "title": "Introduction to metabolomics",
    "section": "The metabolome is constantly changing",
    "text": "The metabolome is constantly changing"
  },
  {
    "objectID": "lectures/01_intro.html#how-is-metabolomics-different-from-targeted-analyses",
    "href": "lectures/01_intro.html#how-is-metabolomics-different-from-targeted-analyses",
    "title": "Introduction to metabolomics",
    "section": "How is metabolomics different from targeted analyses?",
    "text": "How is metabolomics different from targeted analyses?\n\n\nMetabolomics:\n\n100s-1,000s of analytes\nWork on the back end\nComparative (i.e.¬†relative concentration)\n\n\nTargeted analyses:\n\n1-20 analytes\nWork on the front end\nQuantitative (i.e.¬†absolute concentration)"
  },
  {
    "objectID": "lectures/01_intro.html#metabolomics-workflow",
    "href": "lectures/01_intro.html#metabolomics-workflow",
    "title": "Introduction to metabolomics",
    "section": "Metabolomics workflow",
    "text": "Metabolomics workflow"
  },
  {
    "objectID": "lectures/01_intro.html#metabolomics-is-a-comparative-analysis",
    "href": "lectures/01_intro.html#metabolomics-is-a-comparative-analysis",
    "title": "Introduction to metabolomics",
    "section": "Metabolomics is a comparative analysis",
    "text": "Metabolomics is a comparative analysis\n\n\nWhat can food scientists use metabolomics for?\nIf you have specific compounds of interest, develop a targeted method!"
  },
  {
    "objectID": "lectures/01_intro.html#what-do-we-want-to-compare",
    "href": "lectures/01_intro.html#what-do-we-want-to-compare",
    "title": "Introduction to metabolomics",
    "section": "What do we want to compare?",
    "text": "What do we want to compare?\n\n\nIt‚Äôs critical to select comparable samples as our approach is comparative.\nFoods: plants, animal products, raw ingredients, finished product\nBiological sample: plasma, urine, tissue, other fluids, cells"
  },
  {
    "objectID": "lectures/01_intro.html#preparation-dictates-compounds-detected",
    "href": "lectures/01_intro.html#preparation-dictates-compounds-detected",
    "title": "Introduction to metabolomics",
    "section": "Preparation dictates compounds detected",
    "text": "Preparation dictates compounds detected\n\n\n\nYou can only detect what you present to an instrument for analysis\nSample prep depends on intended method of analysis (e.g., water extraction, polar compounds; non-polar extraction, non-polar compounds)\nDilute, centrifuge/filter, inject (e.g.¬†urine, juice, olive oil)"
  },
  {
    "objectID": "lectures/01_intro.html#collect-comprehensive-metabolite-data",
    "href": "lectures/01_intro.html#collect-comprehensive-metabolite-data",
    "title": "Introduction to metabolomics",
    "section": "Collect comprehensive metabolite data",
    "text": "Collect comprehensive metabolite data\n\n3 most popular methods for analysis:\n\nLiquid-chromatography, mass spectrometry (LC-MS)\nGas chromatpgrahy, MS (GC-MS)\nNuclear magnetic resonance spectroscopy (NMR)\n\n\nAll methods have benefits and drawbacks"
  },
  {
    "objectID": "lectures/01_intro.html#convert-spectral-data-into-feature-table",
    "href": "lectures/01_intro.html#convert-spectral-data-into-feature-table",
    "title": "Introduction to metabolomics",
    "section": "Convert spectral data into feature table",
    "text": "Convert spectral data into feature table\n\n\n\nFrom raw spectra, ions are selected, chromatograms drawn, peaks detected, masses and retention times aligned, features dereplicated\nResult is a data file that includes m/z, retention time, compound identifier (usually mz_rt), and relative abundance of each feature in each sample\nWith MZmine, samples are columns, features are rows"
  },
  {
    "objectID": "lectures/01_intro.html#use-statistics-and-chemometrics-to-understand-group-differences",
    "href": "lectures/01_intro.html#use-statistics-and-chemometrics-to-understand-group-differences",
    "title": "Introduction to metabolomics",
    "section": "Use statistics and chemometrics to understand group differences",
    "text": "Use statistics and chemometrics to understand group differences\n\n\n\nSignificance testing (e.g., t-test, Wilcoxon rank sum test, ANOVA)\nUnsupervised analyses (e.g., PCA, hierarchical clustering)\nSupervised analyses (e.g., PLS-DA or PLS-R, random forest)"
  },
  {
    "objectID": "lectures/01_intro.html#what-metabolites-do-we-have",
    "href": "lectures/01_intro.html#what-metabolites-do-we-have",
    "title": "Introduction to metabolomics",
    "section": "What metabolites do we have?",
    "text": "What metabolites do we have?\n\n\n\nSearching publicly available databases (e.g., HMDB, METLIN, GNPS) at the MS1 and MS2 level\nConduct MS/MS experiments\nComparison with authentic standards"
  },
  {
    "objectID": "lectures/01_intro.html#putting-findings-into-a-broader-context",
    "href": "lectures/01_intro.html#putting-findings-into-a-broader-context",
    "title": "Introduction to metabolomics",
    "section": "Putting findings into a broader context",
    "text": "Putting findings into a broader context\n\n\n\nUnderstanding which metabolic pathways are most deregulated\nTypically for enzymatic pathways\nRequires compound IDs (a big limitation)"
  },
  {
    "objectID": "lectures/01_intro.html#ensure-findings-are-real-and-reproducible",
    "href": "lectures/01_intro.html#ensure-findings-are-real-and-reproducible",
    "title": "Introduction to metabolomics",
    "section": "Ensure findings are real and reproducible",
    "text": "Ensure findings are real and reproducible\n\n\n\nMass spectrometry is not inherently quantitative (i.e., if the intensity of analyte A is higher than analyte B, it doesn‚Äôt necessarily mean there is more of A than B)\nKnowing the absolute concentration allows comparison with literature/other data\nValidation in a separate sample set ensures robustness"
  },
  {
    "objectID": "lectures/01_intro.html#metabolomics-workflow-1",
    "href": "lectures/01_intro.html#metabolomics-workflow-1",
    "title": "Introduction to metabolomics",
    "section": "Metabolomics workflow",
    "text": "Metabolomics workflow\n\n\n\n\n¬© Jessica Cooperstone, 2024"
  },
  {
    "objectID": "lectures/03_lc-ms.html#chromatography",
    "href": "lectures/03_lc-ms.html#chromatography",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Chromatography",
    "text": "Chromatography\n\n\nThe process of separating components of a mixture\nBased on an analytes‚Äô difference in partitioning between a stationary and mobile phases\n\n\n\nhttps://en.wikipedia.org/wiki/Chromatography"
  },
  {
    "objectID": "lectures/03_lc-ms.html#types-of-liquid-chromatography",
    "href": "lectures/03_lc-ms.html#types-of-liquid-chromatography",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Types of liquid chromatography",
    "text": "Types of liquid chromatography\n\n\n\n\nReversed phase\nNormal phase\nHILIC (hydrophilic interaction liquid chromatography)"
  },
  {
    "objectID": "lectures/03_lc-ms.html#mass-spectrometry",
    "href": "lectures/03_lc-ms.html#mass-spectrometry",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Mass spectrometry",
    "text": "Mass spectrometry"
  },
  {
    "objectID": "lectures/03_lc-ms.html#specifics-about-ms",
    "href": "lectures/03_lc-ms.html#specifics-about-ms",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Specifics about MS",
    "text": "Specifics about MS\n\n\nRequires gas phase ionization (no charge, no signal!)\nDifferent ionization methods (e.g., ESI vs.¬†APCI)\nInherently non-quantitative\n\nA higher signal for compound A than B doesn‚Äôt necessarily mean compound A is present in higher concentration\n\nIon suppression in complex mixtures"
  },
  {
    "objectID": "lectures/03_lc-ms.html#mass-analyzer-time-of-flight-tof-or-qtof",
    "href": "lectures/03_lc-ms.html#mass-analyzer-time-of-flight-tof-or-qtof",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Mass analyzer: time of flight (TOF or QTOF)",
    "text": "Mass analyzer: time of flight (TOF or QTOF)\n Allen and McWhinney, Clin Biochem Red 2019"
  },
  {
    "objectID": "lectures/03_lc-ms.html#mass-analyzer-orbitrap",
    "href": "lectures/03_lc-ms.html#mass-analyzer-orbitrap",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Mass analyzer: Orbitrap",
    "text": "Mass analyzer: Orbitrap\n\nhttps://www.youtube.com/watch?v=j2_FVJr9xNk"
  },
  {
    "objectID": "lectures/03_lc-ms.html#ms-output",
    "href": "lectures/03_lc-ms.html#ms-output",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "MS output",
    "text": "MS output"
  },
  {
    "objectID": "lectures/03_lc-ms.html#ms-data-is-really-3d",
    "href": "lectures/03_lc-ms.html#ms-data-is-really-3d",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "MS data is really 3D",
    "text": "MS data is really 3D"
  },
  {
    "objectID": "lectures/03_lc-ms.html#different-ways-to-express-a-mass",
    "href": "lectures/03_lc-ms.html#different-ways-to-express-a-mass",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Different ways to express a mass",
    "text": "Different ways to express a mass\n\n\nNominal mass: sum of the integer masses of the constituent elements of a molecule (C=12, H=1)\nMonoisotopic mass: mass of a molecular given empirical formulate calculated using the exact mass of the most abundance isotope of each element (C=12.0000, H=1.0078)\nAverage mass: mass of a molecule given empirical formulate calculated using the weighted average mass for each element by isotopic abundance (C=12.0112, H=1.00797)\nC20H42\n\nNominal mass: (20 √ó 12) + (42 √ó 1) = 282\nMonoisotopic mass: (20 √ó 12.0000) + (42 √ó 1.0078) = 282.3276\nAverage mass: (20 √ó 12.0112) + (42 √ó 1.00797) = 282.5587"
  },
  {
    "objectID": "lectures/03_lc-ms.html#try-calculating-the-monoisotopic-mass",
    "href": "lectures/03_lc-ms.html#try-calculating-the-monoisotopic-mass",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Try calculating the monoisotopic mass",
    "text": "Try calculating the monoisotopic mass\n\n\nTomatidine C27H45NO2"
  },
  {
    "objectID": "lectures/03_lc-ms.html#example-rutin",
    "href": "lectures/03_lc-ms.html#example-rutin",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Example: Rutin",
    "text": "Example: Rutin"
  },
  {
    "objectID": "lectures/03_lc-ms.html#compare-accurate-mass-to-exact-mass-towards-compound-id",
    "href": "lectures/03_lc-ms.html#compare-accurate-mass-to-exact-mass-towards-compound-id",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Compare accurate mass to exact mass towards compound ID",
    "text": "Compare accurate mass to exact mass towards compound ID\n\n\n\n\n\nExact mass is calculated (theoretical)\nTheoretical mass is experimental determined (observed)\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\nrutin observed (positive mode): 611.1613 m/z\nrutin theoretical: 610.1534"
  },
  {
    "objectID": "lectures/03_lc-ms.html#why-you-need-high-mass-accuracy-for-metabolomics",
    "href": "lectures/03_lc-ms.html#why-you-need-high-mass-accuracy-for-metabolomics",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Why you need high mass accuracy for metabolomics",
    "text": "Why you need high mass accuracy for metabolomics"
  },
  {
    "objectID": "lectures/03_lc-ms.html#standards-for-identity-reporting",
    "href": "lectures/03_lc-ms.html#standards-for-identity-reporting",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Standards for identity reporting",
    "text": "Standards for identity reporting\n\nFig. adapted from Sumner et al., Metabolomics 2007"
  },
  {
    "objectID": "lectures/03_lc-ms.html#converting-raw-spectra-into-a-feature-table-i.e.-pre-processing",
    "href": "lectures/03_lc-ms.html#converting-raw-spectra-into-a-feature-table-i.e.-pre-processing",
    "title": "LC-MS data acquisition and pre-processing",
    "section": "Converting raw spectra into a feature table (i.e., pre-processing)",
    "text": "Converting raw spectra into a feature table (i.e., pre-processing)\n\n\n\n\nVendor software:\n\nAgilent: Profinder\nWaters: Progenesis\nThermo: Compound Discoverer\nBruker: MetaboScape\nShimadzu: LabSolutions\n\nOpen source software:\n\nMZmine\nMSDial\nXCMS\nMetaboAnalyst\n\n\n\n\n\n\n\nCastillo et al., Chemometrics Intelligent Lab Systems 2011\n\n\n\n¬© Jessica Cooperstone, 2024"
  },
  {
    "objectID": "activities/05_R.html",
    "href": "activities/05_R.html",
    "title": "Data analysis with R",
    "section": "",
    "text": "We aren‚Äôt going to go over this in the workshop, but I wanted to show you an example of the workflow my team might use for doing the first pass analysis of metabolomics data. We are going to use the feature table directly from MZmine (without any filtering) and will conduct our filtering and analysis in R.\n\n\n\nlibrary(factoextra) # visualizing PCA results\nlibrary(glue) # for easy pasting\nlibrary(plotly) # quick interactive plots\nlibrary(proxyC) # more efficient large matrices\nlibrary(data.table) # easy transposing\nlibrary(janitor) # for cleaning names and checking duplicates\nlibrary(notame) # for collapsing ions coming from the same metabolite\nlibrary(doParallel) # for parallelizing notame specifically\nlibrary(patchwork) # for making multi-panel plots\nlibrary(rstatix) # for additional univariate functionality\nlibrary(philentropy) # for hierarchical clustering\nlibrary(ggdendro) # for hierarchical clustering plotting\nlibrary(ropls) # for pls\n\n# this is at the end hoping that the default select will be that from dplyr\nlibrary(tidyverse) # for everything\n\nOnce you get deconvoluted data from MZmine or similar programs, you need to wrangle your data in such a way that you can conduct your analysis on it.\n\n\n\nFirst we want to read in our raw data. The code here is to read in data directly from MZmine.\n\nmetabdata &lt;- read_csv(file = \"data/Feature_list_MZmine_2560.csv\",\n                      col_names = TRUE) # has headers\n\nNew names:\nRows: 2560 Columns: 49\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" dbl\n(48): row ID, row m/z, row retention time, HATS_402_041.mzML Peak height... lgl\n(1): ...49\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...49`\n\ndim(metabdata)\n\n[1] 2560   49\n\n# look at beginning of the dataset\nmetabdata[1:8, 1:8]\n\n# A tibble: 8 √ó 8\n  `row ID` `row m/z` `row retention time` `HATS_402_041.mzML Peak height`\n     &lt;dbl&gt;     &lt;dbl&gt;                &lt;dbl&gt;                           &lt;dbl&gt;\n1     3106      100.                2.78                           26623.\n2      364      101.                0.611                         105494.\n3      453      102.                0.641                         413981.\n4     5568      103.                3.98                            9505.\n5     2723      103.                2.54                          196964.\n6     1424      104.                1.07                           75342.\n7      397      104.                0.630                         911384.\n8      426      104.                0.629                        2498815 \n# ‚Ñπ 4 more variables: `HATS_403_037.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_404_040.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_401_022.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_408_043.mzML Peak height` &lt;dbl&gt;\n\n# how many features do we have?\nnrow(metabdata)\n\n[1] 2560\n\n\nNote there is no metadata included in this file. Just m/z, retention time, and a column for each sample, where values are peak heights. We are using peak height instead of peak area because it is less dependent on bad peak shape which you get sometimes with metabolomics.\n\ncolnames(metabdata)\n\n [1] \"row ID\"                          \"row m/z\"                        \n [3] \"row retention time\"              \"HATS_402_041.mzML Peak height\"  \n [5] \"HATS_403_037.mzML Peak height\"   \"HATS_404_040.mzML Peak height\"  \n [7] \"HATS_401_022.mzML Peak height\"   \"HATS_408_043.mzML Peak height\"  \n [9] \"HATS_405_025.mzML Peak height\"   \"HATS_406_055.mzML Peak height\"  \n[11] \"HATS_407_059.mzML Peak height\"   \"HATS_412_019.mzML Peak height\"  \n[13] \"HATS_409_056.mzML Peak height\"   \"HATS_410_018.mzML Peak height\"  \n[15] \"HATS_411_044.mzML Peak height\"   \"LA2213_602_035.mzML Peak height\"\n[17] \"LA2213_603_045.mzML Peak height\" \"LA2213_601_051.mzML Peak height\"\n[19] \"LA2213_604_016.mzML Peak height\" \"LA2213_605_017.mzML Peak height\"\n[21] \"LA2213_607_032.mzML Peak height\" \"LA2213_608_024.mzML Peak height\"\n[23] \"LA2213_606_033.mzML Peak height\" \"LA2213_609_058.mzML Peak height\"\n[25] \"LA2213_610_050.mzML Peak height\" \"LA2213_612_046.mzML Peak height\"\n[27] \"OH8243_801_036.mzML Peak height\" \"OH8243_802_026.mzML Peak height\"\n[29] \"OH8243_803_030.mzML Peak height\" \"OH8243_805_057.mzML Peak height\"\n[31] \"LA2213_611_061.mzML Peak height\" \"OH8243_804_029.mzML Peak height\"\n[33] \"OH8243_806_027.mzML Peak height\" \"OH8243_809_063.mzML Peak height\"\n[35] \"OH8243_807_053.mzML Peak height\" \"OH8243_810_049.mzML Peak height\"\n[37] \"OH8243_808_038.mzML Peak height\" \"OH8243_812_060.mzML Peak height\"\n[39] \"OH8243_811_028.mzML Peak height\" \"PB_02_006.mzML Peak height\"     \n[41] \"PB_03_007.mzML Peak height\"      \"PB_01_005.mzML Peak height\"     \n[43] \"QC_02_031.mzML Peak height\"      \"QC_04_047.mzML Peak height\"     \n[45] \"QC_01_023.mzML Peak height\"      \"QC_03_039.mzML Peak height\"     \n[47] \"QC_05_054.mzML Peak height\"      \"QC_06_062.mzML Peak height\"     \n[49] \"...49\"                          \n\n\nIt looks like we have an extra blank column at the end - if we look at our raw data we can see OH9243_811_028 is the last sample, so I am going to remove the last column\n\nmetabdata &lt;- metabdata[,-49]\n\ncolnames(metabdata)\n\n [1] \"row ID\"                          \"row m/z\"                        \n [3] \"row retention time\"              \"HATS_402_041.mzML Peak height\"  \n [5] \"HATS_403_037.mzML Peak height\"   \"HATS_404_040.mzML Peak height\"  \n [7] \"HATS_401_022.mzML Peak height\"   \"HATS_408_043.mzML Peak height\"  \n [9] \"HATS_405_025.mzML Peak height\"   \"HATS_406_055.mzML Peak height\"  \n[11] \"HATS_407_059.mzML Peak height\"   \"HATS_412_019.mzML Peak height\"  \n[13] \"HATS_409_056.mzML Peak height\"   \"HATS_410_018.mzML Peak height\"  \n[15] \"HATS_411_044.mzML Peak height\"   \"LA2213_602_035.mzML Peak height\"\n[17] \"LA2213_603_045.mzML Peak height\" \"LA2213_601_051.mzML Peak height\"\n[19] \"LA2213_604_016.mzML Peak height\" \"LA2213_605_017.mzML Peak height\"\n[21] \"LA2213_607_032.mzML Peak height\" \"LA2213_608_024.mzML Peak height\"\n[23] \"LA2213_606_033.mzML Peak height\" \"LA2213_609_058.mzML Peak height\"\n[25] \"LA2213_610_050.mzML Peak height\" \"LA2213_612_046.mzML Peak height\"\n[27] \"OH8243_801_036.mzML Peak height\" \"OH8243_802_026.mzML Peak height\"\n[29] \"OH8243_803_030.mzML Peak height\" \"OH8243_805_057.mzML Peak height\"\n[31] \"LA2213_611_061.mzML Peak height\" \"OH8243_804_029.mzML Peak height\"\n[33] \"OH8243_806_027.mzML Peak height\" \"OH8243_809_063.mzML Peak height\"\n[35] \"OH8243_807_053.mzML Peak height\" \"OH8243_810_049.mzML Peak height\"\n[37] \"OH8243_808_038.mzML Peak height\" \"OH8243_812_060.mzML Peak height\"\n[39] \"OH8243_811_028.mzML Peak height\" \"PB_02_006.mzML Peak height\"     \n[41] \"PB_03_007.mzML Peak height\"      \"PB_01_005.mzML Peak height\"     \n[43] \"QC_02_031.mzML Peak height\"      \"QC_04_047.mzML Peak height\"     \n[45] \"QC_01_023.mzML Peak height\"      \"QC_03_039.mzML Peak height\"     \n[47] \"QC_05_054.mzML Peak height\"      \"QC_06_062.mzML Peak height\"     \n\n\n\n\n\nYou might have deconvoluted more data than you plan to use in your analysis. For example, you may want to exclude the first bit and last bit of your run, since you do not expect to have good reproducibility in those areas.\nHere, we are filtering to only include features that elute between 0.5-7.5 min of this 10 min run. Let‚Äôs check what we have.\n\nrange(metabdata$`row retention time`)\n\n[1] 0.5184004 7.4863230\n\n\nOur data is already filtered for our desired retention time range, so we don‚Äôt need to do anything. Below is some code you could use to filter if you needed to.\n\nmetabdata_RTfilt &lt;- metabdata %&gt;%\n  filter(between(`row retention time`, 0.5, 7.5))\n\n# did it work?\nrange(metabdata_RTfilt$`row retention time`)\n\n# how many features do we have?\ndim(metabdata_RTfilt)"
  },
  {
    "objectID": "activities/05_R.html#read-in-data",
    "href": "activities/05_R.html#read-in-data",
    "title": "Data analysis with R",
    "section": "",
    "text": "First we want to read in our raw data. The code here is to read in data directly from MZmine.\n\nmetabdata &lt;- read_csv(file = \"data/Feature_list_MZmine_2560.csv\",\n                      col_names = TRUE) # has headers\n\nNew names:\nRows: 2560 Columns: 49\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" dbl\n(48): row ID, row m/z, row retention time, HATS_402_041.mzML Peak height... lgl\n(1): ...49\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...49`\n\ndim(metabdata)\n\n[1] 2560   49\n\n# look at beginning of the dataset\nmetabdata[1:8, 1:8]\n\n# A tibble: 8 √ó 8\n  `row ID` `row m/z` `row retention time` `HATS_402_041.mzML Peak height`\n     &lt;dbl&gt;     &lt;dbl&gt;                &lt;dbl&gt;                           &lt;dbl&gt;\n1     3106      100.                2.78                           26623.\n2      364      101.                0.611                         105494.\n3      453      102.                0.641                         413981.\n4     5568      103.                3.98                            9505.\n5     2723      103.                2.54                          196964.\n6     1424      104.                1.07                           75342.\n7      397      104.                0.630                         911384.\n8      426      104.                0.629                        2498815 \n# ‚Ñπ 4 more variables: `HATS_403_037.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_404_040.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_401_022.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_408_043.mzML Peak height` &lt;dbl&gt;\n\n# how many features do we have?\nnrow(metabdata)\n\n[1] 2560\n\n\nNote there is no metadata included in this file. Just m/z, retention time, and a column for each sample, where values are peak heights. We are using peak height instead of peak area because it is less dependent on bad peak shape which you get sometimes with metabolomics.\n\ncolnames(metabdata)\n\n [1] \"row ID\"                          \"row m/z\"                        \n [3] \"row retention time\"              \"HATS_402_041.mzML Peak height\"  \n [5] \"HATS_403_037.mzML Peak height\"   \"HATS_404_040.mzML Peak height\"  \n [7] \"HATS_401_022.mzML Peak height\"   \"HATS_408_043.mzML Peak height\"  \n [9] \"HATS_405_025.mzML Peak height\"   \"HATS_406_055.mzML Peak height\"  \n[11] \"HATS_407_059.mzML Peak height\"   \"HATS_412_019.mzML Peak height\"  \n[13] \"HATS_409_056.mzML Peak height\"   \"HATS_410_018.mzML Peak height\"  \n[15] \"HATS_411_044.mzML Peak height\"   \"LA2213_602_035.mzML Peak height\"\n[17] \"LA2213_603_045.mzML Peak height\" \"LA2213_601_051.mzML Peak height\"\n[19] \"LA2213_604_016.mzML Peak height\" \"LA2213_605_017.mzML Peak height\"\n[21] \"LA2213_607_032.mzML Peak height\" \"LA2213_608_024.mzML Peak height\"\n[23] \"LA2213_606_033.mzML Peak height\" \"LA2213_609_058.mzML Peak height\"\n[25] \"LA2213_610_050.mzML Peak height\" \"LA2213_612_046.mzML Peak height\"\n[27] \"OH8243_801_036.mzML Peak height\" \"OH8243_802_026.mzML Peak height\"\n[29] \"OH8243_803_030.mzML Peak height\" \"OH8243_805_057.mzML Peak height\"\n[31] \"LA2213_611_061.mzML Peak height\" \"OH8243_804_029.mzML Peak height\"\n[33] \"OH8243_806_027.mzML Peak height\" \"OH8243_809_063.mzML Peak height\"\n[35] \"OH8243_807_053.mzML Peak height\" \"OH8243_810_049.mzML Peak height\"\n[37] \"OH8243_808_038.mzML Peak height\" \"OH8243_812_060.mzML Peak height\"\n[39] \"OH8243_811_028.mzML Peak height\" \"PB_02_006.mzML Peak height\"     \n[41] \"PB_03_007.mzML Peak height\"      \"PB_01_005.mzML Peak height\"     \n[43] \"QC_02_031.mzML Peak height\"      \"QC_04_047.mzML Peak height\"     \n[45] \"QC_01_023.mzML Peak height\"      \"QC_03_039.mzML Peak height\"     \n[47] \"QC_05_054.mzML Peak height\"      \"QC_06_062.mzML Peak height\"     \n[49] \"...49\"                          \n\n\nIt looks like we have an extra blank column at the end - if we look at our raw data we can see OH9243_811_028 is the last sample, so I am going to remove the last column\n\nmetabdata &lt;- metabdata[,-49]\n\ncolnames(metabdata)\n\n [1] \"row ID\"                          \"row m/z\"                        \n [3] \"row retention time\"              \"HATS_402_041.mzML Peak height\"  \n [5] \"HATS_403_037.mzML Peak height\"   \"HATS_404_040.mzML Peak height\"  \n [7] \"HATS_401_022.mzML Peak height\"   \"HATS_408_043.mzML Peak height\"  \n [9] \"HATS_405_025.mzML Peak height\"   \"HATS_406_055.mzML Peak height\"  \n[11] \"HATS_407_059.mzML Peak height\"   \"HATS_412_019.mzML Peak height\"  \n[13] \"HATS_409_056.mzML Peak height\"   \"HATS_410_018.mzML Peak height\"  \n[15] \"HATS_411_044.mzML Peak height\"   \"LA2213_602_035.mzML Peak height\"\n[17] \"LA2213_603_045.mzML Peak height\" \"LA2213_601_051.mzML Peak height\"\n[19] \"LA2213_604_016.mzML Peak height\" \"LA2213_605_017.mzML Peak height\"\n[21] \"LA2213_607_032.mzML Peak height\" \"LA2213_608_024.mzML Peak height\"\n[23] \"LA2213_606_033.mzML Peak height\" \"LA2213_609_058.mzML Peak height\"\n[25] \"LA2213_610_050.mzML Peak height\" \"LA2213_612_046.mzML Peak height\"\n[27] \"OH8243_801_036.mzML Peak height\" \"OH8243_802_026.mzML Peak height\"\n[29] \"OH8243_803_030.mzML Peak height\" \"OH8243_805_057.mzML Peak height\"\n[31] \"LA2213_611_061.mzML Peak height\" \"OH8243_804_029.mzML Peak height\"\n[33] \"OH8243_806_027.mzML Peak height\" \"OH8243_809_063.mzML Peak height\"\n[35] \"OH8243_807_053.mzML Peak height\" \"OH8243_810_049.mzML Peak height\"\n[37] \"OH8243_808_038.mzML Peak height\" \"OH8243_812_060.mzML Peak height\"\n[39] \"OH8243_811_028.mzML Peak height\" \"PB_02_006.mzML Peak height\"     \n[41] \"PB_03_007.mzML Peak height\"      \"PB_01_005.mzML Peak height\"     \n[43] \"QC_02_031.mzML Peak height\"      \"QC_04_047.mzML Peak height\"     \n[45] \"QC_01_023.mzML Peak height\"      \"QC_03_039.mzML Peak height\"     \n[47] \"QC_05_054.mzML Peak height\"      \"QC_06_062.mzML Peak height\""
  },
  {
    "objectID": "activities/05_R.html#rt-filter-if-necessary",
    "href": "activities/05_R.html#rt-filter-if-necessary",
    "title": "Data analysis with R",
    "section": "",
    "text": "You might have deconvoluted more data than you plan to use in your analysis. For example, you may want to exclude the first bit and last bit of your run, since you do not expect to have good reproducibility in those areas.\nHere, we are filtering to only include features that elute between 0.5-7.5 min of this 10 min run. Let‚Äôs check what we have.\n\nrange(metabdata$`row retention time`)\n\n[1] 0.5184004 7.4863230\n\n\nOur data is already filtered for our desired retention time range, so we don‚Äôt need to do anything. Below is some code you could use to filter if you needed to.\n\nmetabdata_RTfilt &lt;- metabdata %&gt;%\n  filter(between(`row retention time`, 0.5, 7.5))\n\n# did it work?\nrange(metabdata_RTfilt$`row retention time`)\n\n# how many features do we have?\ndim(metabdata_RTfilt)"
  },
  {
    "objectID": "activities/05_R.html#create-mz_rt",
    "href": "activities/05_R.html#create-mz_rt",
    "title": "Data analysis with R",
    "section": "Create mz_rt",
    "text": "Create mz_rt\nThis creates a unique identifier for each feature using its mass-to-charge ratio (m/z) and retention time (RT).\n\nMZ_RT &lt;- metabdata %&gt;%\n  rename(mz = `row m/z`,\n         rt = `row retention time`,\n         row_ID = `row ID`) %&gt;%\n  unite(mz_rt, c(mz, rt), remove = FALSE) %&gt;% # Combine m/z & rt with _ in between\n  select(row_ID, mz, rt, mz_rt, everything()) # reorder and move row_ID to front\n\n# how does our df look?\nMZ_RT[1:8, 1:8]\n\n# A tibble: 8 √ó 8\n  row_ID    mz    rt mz_rt         HATS_402_041.mzML Pe‚Ä¶¬π HATS_403_037.mzML Pe‚Ä¶¬≤\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;                  &lt;dbl&gt;\n1   3106  100. 2.78  100.11203254‚Ä¶                 26623.                 45481.\n2    364  101. 0.611 101.07092628‚Ä¶                105494.                104196.\n3    453  102. 0.641 102.05495781‚Ä¶                413981.                413246.\n4   5568  103. 3.98  103.05420188‚Ä¶                  9505.                 24041.\n5   2723  103. 2.54  103.05422465‚Ä¶                196964.                232544.\n6   1424  104. 1.07  104.05281547‚Ä¶                 75342.                 72664.\n7    397  104. 0.630 104.05926456‚Ä¶                911384.                  3235.\n8    426  104. 0.629 104.07084507‚Ä¶               2498815                3274537.\n# ‚Ñπ abbreviated names: ¬π‚Äã`HATS_402_041.mzML Peak height`,\n#   ¬≤‚Äã`HATS_403_037.mzML Peak height`\n# ‚Ñπ 2 more variables: `HATS_404_040.mzML Peak height` &lt;dbl&gt;,\n#   `HATS_401_022.mzML Peak height` &lt;dbl&gt;"
  },
  {
    "objectID": "activities/05_R.html#clean-up-file-names",
    "href": "activities/05_R.html#clean-up-file-names",
    "title": "Data analysis with R",
    "section": "Clean up file names",
    "text": "Clean up file names\nWe are using str_remove() to remove some information we do not need in our sample names.\n\n# remove stuff from the end of file names, \".mzML Peak height\"\nnew_col_names &lt;- str_remove(colnames(MZ_RT), \".mzML Peak height\")\n\n# did it work?\nnew_col_names\n\n [1] \"row_ID\"         \"mz\"             \"rt\"             \"mz_rt\"         \n [5] \"HATS_402_041\"   \"HATS_403_037\"   \"HATS_404_040\"   \"HATS_401_022\"  \n [9] \"HATS_408_043\"   \"HATS_405_025\"   \"HATS_406_055\"   \"HATS_407_059\"  \n[13] \"HATS_412_019\"   \"HATS_409_056\"   \"HATS_410_018\"   \"HATS_411_044\"  \n[17] \"LA2213_602_035\" \"LA2213_603_045\" \"LA2213_601_051\" \"LA2213_604_016\"\n[21] \"LA2213_605_017\" \"LA2213_607_032\" \"LA2213_608_024\" \"LA2213_606_033\"\n[25] \"LA2213_609_058\" \"LA2213_610_050\" \"LA2213_612_046\" \"OH8243_801_036\"\n[29] \"OH8243_802_026\" \"OH8243_803_030\" \"OH8243_805_057\" \"LA2213_611_061\"\n[33] \"OH8243_804_029\" \"OH8243_806_027\" \"OH8243_809_063\" \"OH8243_807_053\"\n[37] \"OH8243_810_049\" \"OH8243_808_038\" \"OH8243_812_060\" \"OH8243_811_028\"\n[41] \"PB_02_006\"      \"PB_03_007\"      \"PB_01_005\"      \"QC_02_031\"     \n[45] \"QC_04_047\"      \"QC_01_023\"      \"QC_03_039\"      \"QC_05_054\"     \n[49] \"QC_06_062\"     \n\n# assign our new column names to MZ_RT\ncolnames(MZ_RT) &lt;- new_col_names\n\nWhat are our sample names?\n\ncolnames(MZ_RT)\n\n [1] \"row_ID\"         \"mz\"             \"rt\"             \"mz_rt\"         \n [5] \"HATS_402_041\"   \"HATS_403_037\"   \"HATS_404_040\"   \"HATS_401_022\"  \n [9] \"HATS_408_043\"   \"HATS_405_025\"   \"HATS_406_055\"   \"HATS_407_059\"  \n[13] \"HATS_412_019\"   \"HATS_409_056\"   \"HATS_410_018\"   \"HATS_411_044\"  \n[17] \"LA2213_602_035\" \"LA2213_603_045\" \"LA2213_601_051\" \"LA2213_604_016\"\n[21] \"LA2213_605_017\" \"LA2213_607_032\" \"LA2213_608_024\" \"LA2213_606_033\"\n[25] \"LA2213_609_058\" \"LA2213_610_050\" \"LA2213_612_046\" \"OH8243_801_036\"\n[29] \"OH8243_802_026\" \"OH8243_803_030\" \"OH8243_805_057\" \"LA2213_611_061\"\n[33] \"OH8243_804_029\" \"OH8243_806_027\" \"OH8243_809_063\" \"OH8243_807_053\"\n[37] \"OH8243_810_049\" \"OH8243_808_038\" \"OH8243_812_060\" \"OH8243_811_028\"\n[41] \"PB_02_006\"      \"PB_03_007\"      \"PB_01_005\"      \"QC_02_031\"     \n[45] \"QC_04_047\"      \"QC_01_023\"      \"QC_03_039\"      \"QC_05_054\"     \n[49] \"QC_06_062\""
  },
  {
    "objectID": "activities/05_R.html#check-for-duplicates",
    "href": "activities/05_R.html#check-for-duplicates",
    "title": "Data analysis with R",
    "section": "Check for duplicates",
    "text": "Check for duplicates\nSometimes you end up with duplicate data after deconvolution with MZmine. Here, we are going to check for complete, perfect duplicates and remove them. The function get_dupes() is from the package janitor.\nWe don‚Äôt want row_ID to be considered here since those are unique per row.\n\nget_dupes(MZ_RT %&gt;% select(-row_ID))\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: mz, rt, mz_rt, HATS_402_041, HATS_403_037, HATS_404_040, HATS_401_022, HATS_408_043, HATS_405_025, ... and 39 other variables\n\n\n# A tibble: 0 √ó 49\n# ‚Ñπ 49 variables: mz &lt;dbl&gt;, rt &lt;dbl&gt;, mz_rt &lt;chr&gt;, HATS_402_041 &lt;dbl&gt;,\n#   HATS_403_037 &lt;dbl&gt;, HATS_404_040 &lt;dbl&gt;, HATS_401_022 &lt;dbl&gt;,\n#   HATS_408_043 &lt;dbl&gt;, HATS_405_025 &lt;dbl&gt;, HATS_406_055 &lt;dbl&gt;,\n#   HATS_407_059 &lt;dbl&gt;, HATS_412_019 &lt;dbl&gt;, HATS_409_056 &lt;dbl&gt;,\n#   HATS_410_018 &lt;dbl&gt;, HATS_411_044 &lt;dbl&gt;, LA2213_602_035 &lt;dbl&gt;,\n#   LA2213_603_045 &lt;dbl&gt;, LA2213_601_051 &lt;dbl&gt;, LA2213_604_016 &lt;dbl&gt;,\n#   LA2213_605_017 &lt;dbl&gt;, LA2213_607_032 &lt;dbl&gt;, LA2213_608_024 &lt;dbl&gt;, ‚Ä¶\n\n\nWe have no exact duplicate, this is great! I‚Äôm including some code that you can use to remove duplicates if you have them.\n\nMZ_RT %&gt;%\n  filter(mz_rt %in% c()) %&gt;%\n  arrange(mz_rt)\n\n\nMZ_RT_nodupes &lt;- MZ_RT %&gt;%\n  filter(!row_ID %in% c(\"insert your duplicated row_IDs here\"))\n\nThis should remove 5 rows.\n\nnrow(MZ_RT) - nrow(MZ_RT_nodupes) # ok good\n\n\nCV function\nSince base R does not have a function to calculate coefficient of variance, let‚Äôs write one.\n\ncv &lt;- function(x){\n        (sd(x)/mean(x))\n}"
  },
  {
    "objectID": "activities/05_R.html#counting-qcs",
    "href": "activities/05_R.html#counting-qcs",
    "title": "Data analysis with R",
    "section": "Counting QCs",
    "text": "Counting QCs\nSubset QCs and filter features to keep only those that are present in 100% of QCs. You could change this parameter based on your data.\n\n# check dimensions of current df\ndim(MZ_RT)\n\n[1] 2560   49\n\nMZ_RT_QCs &lt;- MZ_RT %&gt;%\n  select(mz_rt, contains(\"QC\")) %&gt;% # select QCs\n  filter(rowSums(is.na(.)) &lt;= 1) # remove rows that have 1 or more NAs\n\n\n# check dimensions of QCs filtered df\ndim(MZ_RT_QCs)\n\n[1] 2560    7\n\n# how many features got removed with this filtering?\nnrow(MZ_RT) - nrow(MZ_RT_QCs)\n\n[1] 0\n\n\nIt looks like we didn‚Äôt actually remove anything by doing this."
  },
  {
    "objectID": "activities/05_R.html#filter-on-qc-cv",
    "href": "activities/05_R.html#filter-on-qc-cv",
    "title": "Data analysis with R",
    "section": "Filter on QC CV",
    "text": "Filter on QC CV\nHere we are removing features that have a CV of more than 30% in the QCs. The rationale is that if a feature cannot be reproducibly measured in samples that are all the same, it should not be included in our analysis.\n\n# calculate CV row-wise (1 means row-wise)\nQC_CV &lt;- apply(MZ_RT_QCs[, 2:ncol(MZ_RT_QCs)], 1, cv)\n\n# bind the CV vector back to the QC df\nMZ_RT_QCs_CV &lt;- cbind(MZ_RT_QCs, QC_CV)\n\n# filter for keeping features with QC_CV &lt;= 0.30 (or 30%)\nMZ_RT_QCs_CVfilt &lt;- MZ_RT_QCs_CV %&gt;%\n  filter(QC_CV &lt;= 0.30)\n\nHow many features did I remove with this CV filtering?\n\nnrow(MZ_RT_QCs) - nrow(MZ_RT_QCs_CVfilt)\n\n[1] 46"
  },
  {
    "objectID": "activities/05_R.html#merge-back-the-rest-of-the-data",
    "href": "activities/05_R.html#merge-back-the-rest-of-the-data",
    "title": "Data analysis with R",
    "section": "Merge back the rest of the data",
    "text": "Merge back the rest of the data\nMZ_RT_QCs_CVfilt only contains the QCs, We want to keep only the rows that are present in this df, and then merge back all of the other samples present in MZ_RT. We will do this by creating a vector that has the mz_rt features we want to keep, and then using filter() and %in% to keep only features that are a part of this list.\n\ndim(MZ_RT_QCs_CVfilt)\n\n[1] 2514    8\n\ndim(MZ_RT)\n\n[1] 2560   49\n\n# make a character vector of the mz_rt features we want to keep\n# i.e., the ones that passed our previous filtering steps\nfeatures_to_keep &lt;- as.character(MZ_RT_QCs_CVfilt$mz_rt)\n\nMZ_RT_filt &lt;- MZ_RT %&gt;%\n  filter(mz_rt %in% features_to_keep)\n\ndim(MZ_RT_filt)\n\n[1] 2514   49\n\nget_dupes(MZ_RT_filt %&gt;% select(mz_rt)) # good no dupes\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: mz_rt\n\n\n# A tibble: 0 √ó 2\n# ‚Ñπ 2 variables: mz_rt &lt;chr&gt;, dupe_count &lt;int&gt;\n\n\nYou should have the same number of features in MZ_RT_QCs_CVfilt as you do in your new filtered df MZ_RT_filt.\n\nall.equal(MZ_RT_QCs_CVfilt$mz_rt, MZ_RT_filt$mz_rt)\n\n[1] TRUE"
  },
  {
    "objectID": "activities/05_R.html#process-blanks",
    "href": "activities/05_R.html#process-blanks",
    "title": "Data analysis with R",
    "section": "Process blanks",
    "text": "Process blanks\nWe want to remove features that are present in our process blanks as they are not coming from compounds present in our samples. In this dataset, there are three process blanks (a sample that includes all the extraction materials, minus the sample, here the tomato was replaced by mass with water) has ‚ÄúPB‚Äù in the sample name.\n\n# grab the name of the column/sample that is the process blank\nstr_subset(colnames(MZ_RT_filt), \"PB\")\n\n[1] \"PB_02_006\" \"PB_03_007\" \"PB_01_005\"\n\n\nCalculate the average value across the QCs, then remove features that are not at least 10x higher in the QCs than in the process blank. To do this we will use apply().\napply(X, MARGIN, FUN,...) where X is your df, MARGIN is 1 for row-wise, and 2 for col-wise, and FUN is your function\n\n# pull avg peak height across QCs\navg_height_QC &lt;- apply(MZ_RT_QCs_CVfilt[, 2:ncol(MZ_RT_QCs_CVfilt)], 1, mean)\n\n# bind back to rest of data\nMZ_RT_filt_QC_avg &lt;- cbind(MZ_RT_filt, avg_height_QC)\n\n# check dimensions\ndim(MZ_RT_filt_QC_avg)\n\n[1] 2514   50\n\n\nPull the name of your process blank, and make a new column that indicates how many fold higher your peak height is in your average QC vs your process blank.\n\n# pull name of process blank so we can remember them\nstr_subset(colnames(MZ_RT_filt), \"PB\")\n\n[1] \"PB_02_006\" \"PB_03_007\" \"PB_01_005\"\n\n# make a new column that has a value of how many fold higher peak height is\n# in QCs as compared to PB\n# here there is only  one PB, but would be better to have &gt; 1 but this is ok\n# then you can avg your PBs together and do the same thing\nMZ_RT_filt_PB &lt;- MZ_RT_filt_QC_avg %&gt;% \n  mutate(avg_height_PB = ((PB_02_006 + PB_01_005 + PB_03_007)/3),\n         fold_higher_in_QC = avg_height_QC/avg_height_PB) %&gt;%\n  select(row_ID, mz_rt, mz, rt, avg_height_QC, avg_height_PB, fold_higher_in_QC)\n\nhead(MZ_RT_filt_PB)\n\n  row_ID                      mz_rt       mz        rt avg_height_QC\n1   3106 100.112032546963_2.7774892 100.1120 2.7774892      40408.18\n2    364 101.070926284391_0.6108881 101.0709 0.6108881      78630.77\n3    453  102.054957817964_0.641104 102.0550 0.6411040     420303.10\n4   5568 103.054201883867_3.9781477 103.0542 3.9781477      15405.02\n5   2723 103.054224658561_2.5438855 103.0542 2.5438855     215913.13\n6   1424  104.052815474906_1.065307 104.0528 1.0653070     119092.77\n  avg_height_PB fold_higher_in_QC\n1     1479.6998         27.308366\n2      719.0611        109.351996\n3     3059.5315        137.374986\n4     1894.2296          8.132607\n5     1617.7699        133.463439\n6        0.0000               Inf\n\n\nWe want to keep features that are at least 10x higher in QCs than process blanks, and we also want to keep Infs, because an Inf indicates that a feature absent in the process blanks (i.e., you get an Inf because you‚Äôre trying to divide by zero).\n\n# keep features that are present at least 10x higher in QCs vs PB\n# or, keep NAs because those are absent in blank\nPB_features_to_keep &lt;- MZ_RT_filt_PB %&gt;%\n  filter(fold_higher_in_QC &gt; 10 | is.infinite(fold_higher_in_QC)) \n\ndim(PB_features_to_keep)\n\n[1] 2409    7\n\n\nHow many features did we remove?\n\nnrow(MZ_RT_filt_QC_avg) - nrow(PB_features_to_keep)\n\n[1] 105\n\n\nRemoved some garbage!\nBind back metdata.\n\nMZ_RT_filt_PBremoved &lt;- MZ_RT_filt_QC_avg %&gt;%\n  filter(mz_rt %in% PB_features_to_keep$mz_rt)\n\nnrow(MZ_RT_filt_PBremoved)\n\n[1] 2409\n\n\nDo we have any duplicate features?\n\nget_dupes(MZ_RT_filt_PBremoved, mz_rt)\n\nNo duplicate combinations found of: mz_rt\n\n\n [1] mz_rt          dupe_count     row_ID         mz             rt            \n [6] HATS_402_041   HATS_403_037   HATS_404_040   HATS_401_022   HATS_408_043  \n[11] HATS_405_025   HATS_406_055   HATS_407_059   HATS_412_019   HATS_409_056  \n[16] HATS_410_018   HATS_411_044   LA2213_602_035 LA2213_603_045 LA2213_601_051\n[21] LA2213_604_016 LA2213_605_017 LA2213_607_032 LA2213_608_024 LA2213_606_033\n[26] LA2213_609_058 LA2213_610_050 LA2213_612_046 OH8243_801_036 OH8243_802_026\n[31] OH8243_803_030 OH8243_805_057 LA2213_611_061 OH8243_804_029 OH8243_806_027\n[36] OH8243_809_063 OH8243_807_053 OH8243_810_049 OH8243_808_038 OH8243_812_060\n[41] OH8243_811_028 PB_02_006      PB_03_007      PB_01_005      QC_02_031     \n[46] QC_04_047      QC_01_023      QC_03_039      QC_05_054      QC_06_062     \n[51] avg_height_QC \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nGood, we shouldn‚Äôt because we handled this already.\nRemove samples that we don‚Äôt need anymore.\n\ncolnames(MZ_RT_filt_PBremoved)\n\n [1] \"row_ID\"         \"mz\"             \"rt\"             \"mz_rt\"         \n [5] \"HATS_402_041\"   \"HATS_403_037\"   \"HATS_404_040\"   \"HATS_401_022\"  \n [9] \"HATS_408_043\"   \"HATS_405_025\"   \"HATS_406_055\"   \"HATS_407_059\"  \n[13] \"HATS_412_019\"   \"HATS_409_056\"   \"HATS_410_018\"   \"HATS_411_044\"  \n[17] \"LA2213_602_035\" \"LA2213_603_045\" \"LA2213_601_051\" \"LA2213_604_016\"\n[21] \"LA2213_605_017\" \"LA2213_607_032\" \"LA2213_608_024\" \"LA2213_606_033\"\n[25] \"LA2213_609_058\" \"LA2213_610_050\" \"LA2213_612_046\" \"OH8243_801_036\"\n[29] \"OH8243_802_026\" \"OH8243_803_030\" \"OH8243_805_057\" \"LA2213_611_061\"\n[33] \"OH8243_804_029\" \"OH8243_806_027\" \"OH8243_809_063\" \"OH8243_807_053\"\n[37] \"OH8243_810_049\" \"OH8243_808_038\" \"OH8243_812_060\" \"OH8243_811_028\"\n[41] \"PB_02_006\"      \"PB_03_007\"      \"PB_01_005\"      \"QC_02_031\"     \n[45] \"QC_04_047\"      \"QC_01_023\"      \"QC_03_039\"      \"QC_05_054\"     \n[49] \"QC_06_062\"      \"avg_height_QC\" \n\nMZ_RT_filt_PBremoved &lt;- MZ_RT_filt_PBremoved %&gt;%\n  select(-PB_02_006, -PB_01_005, -PB_03_007, -avg_height_QC)"
  },
  {
    "objectID": "activities/05_R.html#save-your-file",
    "href": "activities/05_R.html#save-your-file",
    "title": "Data analysis with R",
    "section": "Save your file",
    "text": "Save your file\nNow you have a list of features present in your samples after filtering for CV in QCs, and removing all the extraneous columns we added to help us do this, along with removing any process blanks.\n\nwrite_csv(MZ_RT_filt_PBremoved,\n          \"data/post_filtering_6892.csv\")"
  },
  {
    "objectID": "activities/05_R.html#wrangle-sample-names",
    "href": "activities/05_R.html#wrangle-sample-names",
    "title": "Data analysis with R",
    "section": "Wrangle sample names",
    "text": "Wrangle sample names\nHere, the samples are in columns and the features are in rows. Samples are coded so that the first number is the treatment code, and the last code is the run order. We don‚Äôt need pos. We are going to transpose the data so that samples are in rows and features are in tables, and we will also import the metadata about the samples.\n\nmetab_t &lt;- MZ_RT_filt_PBremoved %&gt;%\n  select(-row_ID, -mz, -rt) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"mz_rt\")\n\n# make the first row the column names\ncolnames(metab_t) &lt;- metab_t[1,]\n\n# then remove the first row and rename the column that should be called sample_name\nmetab_t &lt;- metab_t[-1,] %&gt;%\n  rename(sample_name = mz_rt) %&gt;%\n  mutate((across(.cols = 2:ncol(.), .fns = as.numeric)))\n\nmetab_t[1:10, 1:10]\n\n    sample_name 100.112032546963_2.7774892 101.070926284391_0.6108881\n2  HATS_402_041                   26623.42                  105494.08\n3  HATS_403_037                   45481.48                  104196.20\n4  HATS_404_040                   43957.91                  104899.95\n5  HATS_401_022                   47214.43                   90222.12\n6  HATS_408_043                   46941.94                  115545.70\n7  HATS_405_025                   32218.73                   93989.97\n8  HATS_406_055                   31183.91                   95570.40\n9  HATS_407_059                   35303.91                  115281.73\n10 HATS_412_019                   45864.24                  101472.12\n11 HATS_409_056                   23643.99                   97027.94\n   102.054957817964_0.641104 103.054224658561_2.5438855\n2                   413980.8                   196963.9\n3                   413245.7                   232544.1\n4                   410670.9                   233201.0\n5                   422117.5                   201711.9\n6                   379500.5                   317119.0\n7                   449261.5                   196461.6\n8                   433928.6                   172716.3\n9                   332571.8                   233300.6\n10                  392594.0                   236538.0\n11                  390017.1                   159840.7\n   104.052815474906_1.065307 104.059264569687_0.6302757\n2                   75342.02                 911384.440\n3                   72663.66                   3234.576\n4                   98486.46                   4026.670\n5                   78438.71                   3363.092\n6                  128792.77                   3529.039\n7                   69918.09                1575439.000\n8                   64246.62                   2767.284\n9                   88647.18                   2967.580\n10                  78838.38                   3400.089\n11                  71890.95                   3651.530\n   104.070845078818_0.6292385 104.107425371357_0.5997596\n2                     2498815                    4192158\n3                     3274537                    4506788\n4                     3265155                    4466335\n5                     3602852                    4506087\n6                     4000426                    4407152\n7                     2522760                    4389543\n8                     2374514                    4441916\n9                     3509658                    4643859\n10                    3862798                    4476118\n11                    3250134                    4585277\n   104.124821538078_0.6135892\n2                    158717.3\n3                    246090.9\n4                    240655.1\n5                    246382.4\n6                    264137.5\n7                    146173.3\n8                    223540.2\n9                    258637.6\n10                   247997.0\n11                   246773.9\n\n\nAdd in the metadata and make a new column that will indicate whether a sample is a ‚Äúsample‚Äù or a ‚ÄúQC‚Äù. Extract the metadata out of the column names. The metadata we have for the samples we are no longer using (process blanks etc) are removed.\n\nmetab_plus &lt;- metab_t %&gt;%\n  mutate(sample_or_qc = if_else(str_detect(sample_name, \"QC\"), \n                                true = \"QC\", false = \"Sample\")) %&gt;%\n  separate_wider_delim(cols = sample_name, delim = \"_\",\n                       names = c(\"tomato\", \"rep_or_plot\", \"run_order\"),\n                       cols_remove = FALSE) %&gt;%\n  select(sample_name, sample_or_qc, tomato, rep_or_plot, run_order, everything()) %&gt;%\n  mutate(sample_or_qc = as.factor(sample_or_qc),\n         tomato = as.factor(tomato),\n         run_order = as.numeric(run_order))\n         \n\n# how does it look\nmetab_plus[1:5, 1:8]\n\n# A tibble: 5 √ó 8\n  sample_name  sample_or_qc tomato rep_or_plot run_order 100.112032546963_2.77‚Ä¶¬π\n  &lt;chr&gt;        &lt;fct&gt;        &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt;                   &lt;dbl&gt;\n1 HATS_402_041 Sample       HATS   402                41                  26623.\n2 HATS_403_037 Sample       HATS   403                37                  45481.\n3 HATS_404_040 Sample       HATS   404                40                  43958.\n4 HATS_401_022 Sample       HATS   401                22                  47214.\n5 HATS_408_043 Sample       HATS   408                43                  46942.\n# ‚Ñπ abbreviated name: ¬π‚Äã`100.112032546963_2.7774892`\n# ‚Ñπ 2 more variables: `101.070926284391_0.6108881` &lt;dbl&gt;,\n#   `102.054957817964_0.641104` &lt;dbl&gt;\n\n\nGo from wide to long data.\n\nmetab_plus_long &lt;- metab_plus %&gt;%\n  pivot_longer(cols = -c(sample_name, sample_or_qc, tomato, rep_or_plot, run_order,),  # remove metadata\n               names_to = \"mz_rt\",\n               values_to = \"rel_abund\")\n\nglimpse(metab_plus_long)\n\nRows: 101,178\nColumns: 7\n$ sample_name  &lt;chr&gt; \"HATS_402_041\", \"HATS_402_041\", \"HATS_402_041\", \"HATS_402‚Ä¶\n$ sample_or_qc &lt;fct&gt; Sample, Sample, Sample, Sample, Sample, Sample, Sample, S‚Ä¶\n$ tomato       &lt;fct&gt; HATS, HATS, HATS, HATS, HATS, HATS, HATS, HATS, HATS, HAT‚Ä¶\n$ rep_or_plot  &lt;chr&gt; \"402\", \"402\", \"402\", \"402\", \"402\", \"402\", \"402\", \"402\", \"‚Ä¶\n$ run_order    &lt;dbl&gt; 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 4‚Ä¶\n$ mz_rt        &lt;chr&gt; \"100.112032546963_2.7774892\", \"101.070926284391_0.6108881‚Ä¶\n$ rel_abund    &lt;dbl&gt; 26623.416, 105494.080, 413980.780, 196963.940, 75342.016,‚Ä¶\n\n\nAlso add separate columns for mz and rt, and making both numeric.\n\nmetab_plus_long &lt;- metab_plus_long %&gt;%\n  separate_wider_delim(cols = mz_rt,\n                       delim = \"_\",\n                       names = c(\"mz\", \"rt\"),\n                       cols_remove = FALSE) %&gt;%\n  mutate(across(.cols = c(\"mz\", \"rt\"), .fns = as.numeric)) # convert mz and rt to numeric\n\n# how did that go?\nhead(metab_plus_long)\n\n# A tibble: 6 √ó 9\n  sample_name  sample_or_qc tomato rep_or_plot run_order    mz    rt mz_rt      \n  &lt;chr&gt;        &lt;fct&gt;        &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1 HATS_402_041 Sample       HATS   402                41  100. 2.78  100.112032‚Ä¶\n2 HATS_402_041 Sample       HATS   402                41  101. 0.611 101.070926‚Ä¶\n3 HATS_402_041 Sample       HATS   402                41  102. 0.641 102.054957‚Ä¶\n4 HATS_402_041 Sample       HATS   402                41  103. 2.54  103.054224‚Ä¶\n5 HATS_402_041 Sample       HATS   402                41  104. 1.07  104.052815‚Ä¶\n6 HATS_402_041 Sample       HATS   402                41  104. 0.630 104.059264‚Ä¶\n# ‚Ñπ 1 more variable: rel_abund &lt;dbl&gt;"
  },
  {
    "objectID": "activities/05_R.html#data-summaries",
    "href": "activities/05_R.html#data-summaries",
    "title": "Data analysis with R",
    "section": "Data summaries",
    "text": "Data summaries\nWhat mass range do I have?\n\nrange(metab_plus_long$mz)\n\n[1]  100.112 1589.722\n\n\nWhat retention time range do I have?\n\nrange(metab_plus_long$rt)\n\n[1] 0.5184004 7.4507394\n\n\nHow many samples are in each of my meta-data groups?\n\n# make wide data to make some calculations easier\nmetab_wide_meta &lt;- metab_plus_long %&gt;%\n  dplyr::select(-mz, -rt) %&gt;%\n  pivot_wider(names_from = mz_rt,\n              values_from = rel_abund)\n\n# by sample vs QC\nmetab_wide_meta %&gt;%\n  count(sample_or_qc)\n\n# A tibble: 2 √ó 2\n  sample_or_qc     n\n  &lt;fct&gt;        &lt;int&gt;\n1 QC               6\n2 Sample          36\n\n# by tomato\nmetab_wide_meta %&gt;%\n  count(tomato)\n\n# A tibble: 4 √ó 2\n  tomato     n\n  &lt;fct&gt;  &lt;int&gt;\n1 HATS      12\n2 LA2213    12\n3 OH8243    12\n4 QC         6\n\n\nWhat does my data coverage across mz and rt look like?\n\nmetab_plus_long %&gt;%\n  group_by(mz_rt) %&gt;% # so we only have one point per feature\n  ggplot(aes(x = rt, y = mz)) +\n  geom_point(alpha = 0.01) +\n  theme_minimal() +\n  labs(x = \"Retention time (min)\",\n       y = \"Mass to charge ratio (m/z)\",\n       title = \"m/z by retention time plot (all features)\",\n       subtitle = \"C18 reversed phase, positive ionization mode\")\n\n\n\n\nAll of this overlap makes me think we have replication of features.\nDistribution of masses\n\nmetab_plus_long %&gt;%\n  group_by(mz_rt) %&gt;%\n  ggplot(aes(x = mz)) +\n  geom_histogram(bins = 100) +\n  theme_minimal() +\n  labs(x = \"m/z\",\n       y = \"Number of features\",\n       title = \"Distribution of features by mass\")\n\n\n\n\nDistribution of retention times\n\nmetab_plus_long %&gt;%\n  group_by(mz_rt) %&gt;%\n  ggplot(aes(x = rt)) +\n  geom_density() +\n  theme_minimal() +\n  labs(x = \"Retention time\",\n       y = \"Number of features\",\n       title = \"Distribution of features by retention time\")"
  },
  {
    "objectID": "activities/05_R.html#missing-data",
    "href": "activities/05_R.html#missing-data",
    "title": "Data analysis with R",
    "section": "Missing data",
    "text": "Missing data\n\nSurveying missingness\nHow many missing values are there for each feature? In this dataset, missing values are coded as zero.\n\n# all data including QCs\n# how many missing values are there for each feature (row)\nna_by_feature &lt;- rowSums(is.na(MZ_RT_filt_PBremoved)) %&gt;%\n  as.data.frame() %&gt;%\n  rename(missing_values = 1)\n  \n\nna_by_feature %&gt;%\n  ggplot(aes(x = missing_values)) +\n  geom_histogram(bins = 40) + # since 40 samples\n  theme_minimal() + \n  labs(title = \"Number of missing values for each feature\",\n       x = \"Number of missing values\",\n       y = \"How many features have that \\nmany missing values\")\n\n\n\n\nHow many features have no missing values?\n\nna_by_feature %&gt;%\n  count(missing_values == 0)\n\n  missing_values == 0    n\n1               FALSE  628\n2                TRUE 1781\n\n\nHow many missing values are there for each sample?\n\n# all data including QCs\n# how many missing values are there for each feature (row)\nna_by_sample &lt;- colSums(is.na(MZ_RT_filt_PBremoved)) %&gt;%\n  as.data.frame() %&gt;%\n  rename(missing_values = 1) %&gt;%\n  rownames_to_column(var = \"feature\") %&gt;%\n  filter(!feature == \"mz_rt\")\n\nna_by_sample %&gt;%\n  ggplot(aes(x = missing_values)) +\n  geom_histogram(bins = 100) + # \n  theme_minimal() + \n  labs(title = \"Number of missing values for each sample\",\n       x = \"Number of missing values\",\n       y = \"How many samples have that \\nmany missing values\")\n\n\n\n\nWhich features have a lot of missing values?\n\ncontains_NAs_feature &lt;- metab_plus_long %&gt;%\n  group_by(mz_rt) %&gt;%\n  count(is.na(rel_abund)) %&gt;%\n  filter(`is.na(rel_abund)` == TRUE) %&gt;%\n  arrange(desc(n))\n\nhead(contains_NAs_feature)\n\n# A tibble: 6 √ó 3\n# Groups:   mz_rt [6]\n  mz_rt                      `is.na(rel_abund)`     n\n  &lt;chr&gt;                      &lt;lgl&gt;              &lt;int&gt;\n1 743.251838840705_6.577289  TRUE                  28\n2 628.382914819219_4.8645153 TRUE                  27\n3 465.285351543191_4.7723923 TRUE                  26\n4 399.246808492192_4.5366135 TRUE                  25\n5 421.259892834676_4.621516  TRUE                  25\n6 472.341839506857_6.440611  TRUE                  25\n\n\nWhich samples have a lot of missing values?\n\ncontains_NAs_sample &lt;- metab_plus_long %&gt;%\n  group_by(sample_name) %&gt;%\n  count(is.na(rel_abund)) %&gt;%\n  filter(`is.na(rel_abund)` == TRUE) %&gt;%\n  arrange(desc(n))\n\nhead(contains_NAs_sample)\n\n# A tibble: 6 √ó 3\n# Groups:   sample_name [6]\n  sample_name    `is.na(rel_abund)`     n\n  &lt;chr&gt;          &lt;lgl&gt;              &lt;int&gt;\n1 OH8243_805_057 TRUE                 305\n2 OH8243_806_027 TRUE                 300\n3 OH8243_801_036 TRUE                 238\n4 OH8243_810_049 TRUE                 222\n5 OH8243_811_028 TRUE                 221\n6 OH8243_808_038 TRUE                 214\n\n\nAre there any missing values in the QCs? (There shouldn‚Äôt be.)\n\nmetab_QC &lt;- MZ_RT_filt_PBremoved %&gt;%\n  dplyr::select(contains(\"QC\"))\n\nna_by_sample &lt;- colSums(is.na(metab_QC)) %&gt;%\n  as.data.frame() %&gt;%\n  rename(missing_values = 1) %&gt;%\n  rownames_to_column(var = \"feature\") %&gt;%\n  filter(!feature == \"mz_rt\")\n\nsum(na_by_sample$missing_values) # nope\n\n[1] 0\n\n\n\n\nImputing missing values\nThis is an optional step but some downstream analyses don‚Äôt handle missingness well. Here we are imputing missing data with half the lowest value observed for that feature.\n\n# grab only the feature data and leave metadata\nmetab_wide_meta_imputed &lt;- metab_wide_meta %&gt;%\n  dplyr::select(-c(1:5)) # the metadata columns\n\nmetab_wide_meta_imputed[] &lt;- lapply(metab_wide_meta_imputed,\n                                 function(x) ifelse(is.na(x), min(x, na.rm = TRUE)/2, x))\n\n# bind back the metadata\nmetab_wide_meta_imputed &lt;- bind_cols(metab_wide_meta[,1:5], metab_wide_meta_imputed)\n\n# try working from original MZ_RT_filt_PBremoved input file for notame later\nmetab_imputed &lt;- MZ_RT_filt_PBremoved %&gt;%\n  dplyr::select(-row_ID, -mz_rt, -mz, -rt)\n\nmetab_imputed[] &lt;- lapply(metab_imputed,\n                          function(x) ifelse(is.na(x), min(x, na.rm = TRUE)/2, x))\n\n# bind back metadata\nmetab_imputed &lt;- bind_cols (MZ_RT_filt_PBremoved$mz_rt, metab_imputed) %&gt;% # just add back mz_rt\n  rename(mz_rt = 1) # rename first column back to mz_rt\n\nNew names:\n‚Ä¢ `` -&gt; `...1`\n\n\nDid imputing work?\n\n# count missing values\nmetab_wide_meta_imputed %&gt;%\n  dplyr::select(-c(1:5)) %&gt;% # where the metadata is\n  is.na() %&gt;%\n  sum()\n\n[1] 0\n\n\nCreate long imputed dataset.\n\nmetab_long_meta_imputed &lt;- metab_wide_meta_imputed %&gt;%\n  pivot_longer(cols = 6:ncol(.),\n               names_to = \"mz_rt\",\n               values_to = \"rel_abund\")\n\nhead(metab_long_meta_imputed)\n\n# A tibble: 6 √ó 7\n  sample_name  sample_or_qc tomato rep_or_plot run_order mz_rt         rel_abund\n  &lt;chr&gt;        &lt;fct&gt;        &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1 HATS_402_041 Sample       HATS   402                41 100.11203254‚Ä¶    26623.\n2 HATS_402_041 Sample       HATS   402                41 101.07092628‚Ä¶   105494.\n3 HATS_402_041 Sample       HATS   402                41 102.05495781‚Ä¶   413981.\n4 HATS_402_041 Sample       HATS   402                41 103.05422465‚Ä¶   196964.\n5 HATS_402_041 Sample       HATS   402                41 104.05281547‚Ä¶    75342.\n6 HATS_402_041 Sample       HATS   402                41 104.05926456‚Ä¶   911384.\n\n\nLet‚Äôs also make separate mz and rt columns.\n\nmetab_long_meta_imputed &lt;- metab_long_meta_imputed %&gt;%\n  separate_wider_delim(cols = mz_rt,\n                       delim = \"_\",\n                       names = c(\"mz\", \"rt\"),\n                       cols_remove = FALSE)\n\nmetab_long_meta_imputed$mz &lt;- as.numeric(metab_long_meta_imputed$mz)\nmetab_long_meta_imputed$rt &lt;- as.numeric(metab_long_meta_imputed$rt)"
  },
  {
    "objectID": "activities/05_R.html#feature-clustering-with-notame",
    "href": "activities/05_R.html#feature-clustering-with-notame",
    "title": "Data analysis with R",
    "section": "Feature clustering with notame",
    "text": "Feature clustering with notame\nWe want to cluster features that likely come from the same metabolite together, and we can do this using the package notame. You can learn more here.\n\nbrowseVignettes(\"notame\")\n\nLet‚Äôs make a m/z by retention time plot again before we start.\n\n(before_notame &lt;- metab_long_meta_imputed %&gt;%\n  group_by(mz_rt) %&gt;% # so we only have one point per feature\n  ggplot(aes(x = rt, y = mz)) +\n  geom_point(alpha = 0.01) +\n  theme_minimal() +\n  labs(x = \"Retention time (min)\",\n       y = \"Mass to charge ratio (m/z)\",\n       title = \"m/z by retention time plot before notame\",\n       subtitle = \"C18 reverse phase, positive ionization mode\"))\n\n\n\n\n\nWrangling data\nTranspose the wide data for notame and wrangle to the right format. Below is info from the documentation:\n\nData should be a data frame containing the abundances of features in each sample, one row per sample, each feature in a separate column\nFeatures should be a data frame containing information about the features, namely feature name (should be the same as the column name in data), mass and retention time\n\nGoing back to the original imported data and imputing from there seems kind of silly, but I had a lot of problems structuring this data to get find_connections() to run and not throw any errors because of names that weren‚Äôt the same between the features and the data inputs.\nIt is important that for the Data, your first sample is in row 2. The code below will get you there. If you‚Äôre wondering why the code is written this way instead of just using metab_wide_meta_imputed, this is why.\n\n# # create a data frame which is just the original metab data\n# transposed so samples are rows and features are columns\ndata_notame &lt;- data.frame(metab_imputed %&gt;%\n                          t())\n\ndata_notame &lt;- data_notame %&gt;%\n  tibble::rownames_to_column() %&gt;% # change samples from rownames to its own column\n  row_to_names(row_number = 1) # change the feature IDs (mz_rt) from first row obs into column names\n\n# change to results to numeric\n# it is important that the first row of data has the row number 2\n# i don't know why this is but save yourself all the time maria/jess spent figuring out\n# why this wasn't working\ndata_notame &lt;- data_notame %&gt;%\n  mutate(across(-mz_rt, as.numeric))\n\ntibble(data_notame)\n\n# A tibble: 42 √ó 2,410\n   mz_rt    100.112032546963_2.7‚Ä¶¬π 101.070926284391_0.6‚Ä¶¬≤ 102.054957817964_0.6‚Ä¶¬≥\n   &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n 1 HATS_40‚Ä¶                 26623.                105494.                413981.\n 2 HATS_40‚Ä¶                 45481.                104196.                413246.\n 3 HATS_40‚Ä¶                 43958.                104900.                410671.\n 4 HATS_40‚Ä¶                 47214.                 90222.                422118.\n 5 HATS_40‚Ä¶                 46942.                115546.                379500.\n 6 HATS_40‚Ä¶                 32219.                 93990.                449261.\n 7 HATS_40‚Ä¶                 31184.                 95570.                433929.\n 8 HATS_40‚Ä¶                 35304.                115282.                332572.\n 9 HATS_41‚Ä¶                 45864.                101472.                392594 \n10 HATS_40‚Ä¶                 23644.                 97028.                390017.\n# ‚Ñπ 32 more rows\n# ‚Ñπ abbreviated names: ¬π‚Äã`100.112032546963_2.7774892`,\n#   ¬≤‚Äã`101.070926284391_0.6108881`, ¬≥‚Äã`102.054957817964_0.641104`\n# ‚Ñπ 2,406 more variables: `103.054224658561_2.5438855` &lt;dbl&gt;,\n#   `104.052815474906_1.065307` &lt;dbl&gt;, `104.059264569687_0.6302757` &lt;dbl&gt;,\n#   `104.070845078818_0.6292385` &lt;dbl&gt;, `104.107425371357_0.5997596` &lt;dbl&gt;,\n#   `104.124821538078_0.6135892` &lt;dbl&gt;, `104.130633537458_0.60107374` &lt;dbl&gt;, ‚Ä¶\n\n\nCreate df with features.\n\nfeatures &lt;- metab_long_meta_imputed %&gt;%\n  dplyr::select(mz_rt, mz, rt) %&gt;%\n  mutate(across(c(mz, rt), as.numeric)) %&gt;%\n  as.data.frame() %&gt;%\n  distinct()\n\nglimpse(features)\n\nRows: 2,409\nColumns: 3\n$ mz_rt &lt;chr&gt; \"100.112032546963_2.7774892\", \"101.070926284391_0.6108881\", \"102‚Ä¶\n$ mz    &lt;dbl&gt; 100.1120, 101.0709, 102.0550, 103.0542, 104.0528, 104.0593, 104.‚Ä¶\n$ rt    &lt;dbl&gt; 2.7774892, 0.6108881, 0.6411040, 2.5438855, 1.0653070, 0.6302757‚Ä¶\n\nclass(features)\n\n[1] \"data.frame\"\n\n\n\n\nFind connections\nSet cache = TRUE for this chunk since its a bit slow especially if you have a lot of features. Here, this step took 25 min for almost 7K features or 5 min for just over 2K features.\n\nconnection &lt;- find_connections(data = data_notame,\n                               features = features,\n                               corr_thresh = 0.9,\n                               rt_window = 1/60,\n                               name_col = \"mz_rt\",\n                               mz_col = \"mz\",\n                               rt_col = \"rt\")\n\n[1] 100\n[1] 200\n[1] 300\n[1] 400\n[1] 500\n[1] 600\n[1] 700\n[1] 800\n[1] 900\n[1] 1000\n[1] 1100\n[1] 1200\n[1] 1300\n[1] 1400\n[1] 1500\n[1] 1600\n[1] 1700\n[1] 1800\n[1] 1900\n[1] 2000\n[1] 2100\n[1] 2200\n[1] 2300\n[1] 2400\n\n\n\nhead(connection)\n\n                           x                           y       cor     rt_diff\n1 100.112032546963_2.7774892  146.117590231148_2.7758389 0.9828013 -0.00165030\n2 101.070926284391_0.6108881  147.077120604255_0.6086585 0.9835768 -0.00222960\n3 101.070926284391_0.6108881   147.22323215244_0.6100122 0.9045387 -0.00087590\n4 101.070926284391_0.6108881  147.250378998592_0.6080253 0.9604805 -0.00286280\n5  102.054957817964_0.641104 148.061209408145_0.63665396 0.9906094 -0.00445004\n6  102.054957817964_0.641104 149.063907598478_0.63694835 0.9823400 -0.00415565\n   mz_diff\n1 46.00556\n2 46.00619\n3 46.15231\n4 46.17945\n5 46.00625\n6 47.00895\n\n\n\n\nClustering\nNow that we have found all of the features that are connected based on the parameterers we have set, we now need to find clusters.\n\nclusters &lt;- find_clusters(connections = connection, \n                          d_thresh = 0.8)\n\n285 components found\n\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nComponent 100 / 285 \nComponent 200 / 285 \n147 components found\n\nComponent 100 / 147 \n35 components found\n\n7 components found\n\n7 components found\n\n8 components found\n\n7 components found\n\n\nAssign a cluster ID to each feature to keep, and the feature that is picked is the one with the highest median peak intensity across the samples.\n\n# assign a cluster ID to all features\n# clusters are named after feature with highest median peak height\nfeatures_clustered &lt;- assign_cluster_id(data_notame, \n                                        clusters, \n                                        features, \n                                        name_col = \"mz_rt\")\n\nExport out a list of your clusters this way you can use this later during metabolite ID.\n\n# export clustered feature list this way you have it\nwrite_csv(features_clustered,\n          \"data/features_notame-clusters.csv\")\n\nPull data out from the clusters and see how many features we removed/have now.\n\n# lets see how many features are removed when we only keep one feature per cluster\npulled &lt;- pull_clusters(data_notame, features_clustered, name_col = \"mz_rt\")\n\ncluster_data &lt;- pulled$cdata\ncluster_features &lt;- pulled$cfeatures\n\n# how many features did we originally have after filtering?\nnrow(metab_imputed)\n\n[1] 2409\n\n# how many features got removed during clustering?\nnrow(metab_imputed) - nrow(cluster_features)\n\n[1] 1131\n\n# what percentage of the original features were removed?\n((nrow(metab_imputed) - nrow(cluster_features))/nrow(metab_imputed)) * 100\n\n[1] 46.94894\n\n# how many features do we have now?\nnrow(cluster_features)\n\n[1] 1278\n\n\nReduce our dataset to include only our new clusters. cluster_data contains only the retained clusters, while cluster_features tells you also which features are a part of each cluster.\n\n# combined metadata_plus with cluster_features\ncluster_data &lt;- cluster_data %&gt;%\n  rename(sample_name = mz_rt) # since this column is actually sample name\n\n# make a wide df\nmetab_imputed_clustered_wide &lt;- left_join(metab_wide_meta_imputed[,1:5], cluster_data,\n                                          by = \"sample_name\") \n\ndim(metab_imputed_clustered_wide) # we have 2474 features since 4 metadata columns\n\n[1]   42 1283\n\n# make a long/tidy df\nmetab_imputed_clustered_long &lt;- metab_imputed_clustered_wide %&gt;%\n  pivot_longer(cols = 6:ncol(.),\n               names_to = \"mz_rt\",\n               values_to = \"rel_abund\") %&gt;%\n  separate_wider_delim(cols = mz_rt, # make separate columns for mz and rt too\n                       delim = \"_\",\n                       names = c(\"mz\", \"rt\"),\n                       cols_remove = FALSE) %&gt;%\n  mutate(across(.cols = c(\"mz\", \"rt\"), .fns = as.numeric)) # make mz and rt numeric\n\nLet‚Äôs look at a m/z by retention time plot again after clustering.\n\n(after_notame &lt;- metab_imputed_clustered_long %&gt;%\n  group_by(mz_rt) %&gt;% # so we only have one point per feature\n  ggplot(aes(x = rt, y = mz)) +\n  geom_point(alpha = 0.01) +\n  theme_minimal() +\n  labs(x = \"Retention time (min)\",\n       y = \"Mass to charge ratio (m/z)\",\n       title = \"m/z by retention time plot after notame\",\n       subtitle = \"C18 reverse phase, positive ionization mode\"))\n\n\n\n\n\nbefore_notame / after_notame"
  },
  {
    "objectID": "activities/05_R.html#assessing-data-quality",
    "href": "activities/05_R.html#assessing-data-quality",
    "title": "Data analysis with R",
    "section": "Assessing data quality",
    "text": "Assessing data quality\nLet‚Äôs make sure that our data is of good quality.\n\nUntransformed data\nFirst we are going to convert the type of some of the columns to match what we want (e.g., run order converted to numeric, species to factor)\n\ntibble(metab_imputed_clustered_long)\n\n# A tibble: 53,676 √ó 9\n   sample_name  sample_or_qc tomato rep_or_plot run_order    mz    rt mz_rt     \n   &lt;chr&gt;        &lt;fct&gt;        &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1 HATS_402_041 Sample       HATS   402                41  104. 0.630 104.05926‚Ä¶\n 2 HATS_402_041 Sample       HATS   402                41  104. 0.629 104.07084‚Ä¶\n 3 HATS_402_041 Sample       HATS   402                41  104. 0.600 104.10742‚Ä¶\n 4 HATS_402_041 Sample       HATS   402                41  104. 0.614 104.12482‚Ä¶\n 5 HATS_402_041 Sample       HATS   402                41  109. 0.522 108.96194‚Ä¶\n 6 HATS_402_041 Sample       HATS   402                41  110. 0.520 110.00910‚Ä¶\n 7 HATS_402_041 Sample       HATS   402                41  111. 1.12  111.00780‚Ä¶\n 8 HATS_402_041 Sample       HATS   402                41  112. 2.25  112.05052‚Ä¶\n 9 HATS_402_041 Sample       HATS   402                41  112. 0.710 112.05058‚Ä¶\n10 HATS_402_041 Sample       HATS   402                41  112. 0.533 112.08688‚Ä¶\n# ‚Ñπ 53,666 more rows\n# ‚Ñπ 1 more variable: rel_abund &lt;dbl&gt;\n\n# make run_order numeric\nmetab_imputed_clustered_long$run_order &lt;- as.numeric(metab_imputed_clustered_long$run_order)\n\n# make treatment and sample_or_qc a factor (i.e., categorical)\nmetab_imputed_clustered_long$tomato &lt;- as.factor(metab_imputed_clustered_long$tomato)\nmetab_imputed_clustered_long$sample_or_qc &lt;- as.factor(metab_imputed_clustered_long$sample_or_qc)\n\n# did it work?\ntibble(metab_imputed_clustered_long)\n\n# A tibble: 53,676 √ó 9\n   sample_name  sample_or_qc tomato rep_or_plot run_order    mz    rt mz_rt     \n   &lt;chr&gt;        &lt;fct&gt;        &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1 HATS_402_041 Sample       HATS   402                41  104. 0.630 104.05926‚Ä¶\n 2 HATS_402_041 Sample       HATS   402                41  104. 0.629 104.07084‚Ä¶\n 3 HATS_402_041 Sample       HATS   402                41  104. 0.600 104.10742‚Ä¶\n 4 HATS_402_041 Sample       HATS   402                41  104. 0.614 104.12482‚Ä¶\n 5 HATS_402_041 Sample       HATS   402                41  109. 0.522 108.96194‚Ä¶\n 6 HATS_402_041 Sample       HATS   402                41  110. 0.520 110.00910‚Ä¶\n 7 HATS_402_041 Sample       HATS   402                41  111. 1.12  111.00780‚Ä¶\n 8 HATS_402_041 Sample       HATS   402                41  112. 2.25  112.05052‚Ä¶\n 9 HATS_402_041 Sample       HATS   402                41  112. 0.710 112.05058‚Ä¶\n10 HATS_402_041 Sample       HATS   402                41  112. 0.533 112.08688‚Ä¶\n# ‚Ñπ 53,666 more rows\n# ‚Ñπ 1 more variable: rel_abund &lt;dbl&gt;\n\n\nLet‚Äôs make a boxplot to see how the metabolite abundance looks across different samples.\n\nmetab_imputed_clustered_long %&gt;%\n  ggplot(aes(x = sample_name, y = rel_abund, fill = tomato)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90),\n        legend.position = \"none\") +\n  labs(title = \"LC-MS (+) Feature Abundances by Sample\",\n       subtitle = \"Data is unscaled\",\n       y = \"Relative abundance\")\n\n\n\n\nCan‚Äôt really see anything because data is skewed.\n\n\nTransformed data\n\nLog2 transformed\nWe will log2 transform our data.\n\nmetab_imputed_clustered_long_log2 &lt;- metab_imputed_clustered_long %&gt;%\n  mutate(rel_abund = log2(rel_abund))\n\nAnd then plot.\n\nmetab_imputed_clustered_long_log2 %&gt;%\n  ggplot(aes(x = sample_name, y = rel_abund, fill = tomato)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90),\n        legend.position = \"none\") +\n  labs(title = \"LC-MS (+) Feature Abundances by Sample\",\n       subtitle = \"Data is log2 transformed\",\n       y = \"Relative abundance\")\n\n\n\n\nWe can also look at this data by run order to see if we have any overall run order effects visible.\n\nmetab_imputed_clustered_long_log2 %&gt;%\n  mutate(sample_name = fct_reorder(sample_name, run_order)) %&gt;%\n  ggplot(aes(x = sample_name, y = rel_abund, fill = tomato)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(title = \"LC-MS (+) Feature Abundances by Sample\",\n       subtitle = \"Data is log2 transformed\",\n       y = \"Relative abundance\")\n\n\n\n\n\n\nLog10 transformed\nWe will log10 transform our data.\n\nmetab_imputed_clustered_long_log10 &lt;- metab_imputed_clustered_long %&gt;%\n  mutate(rel_abund = log10(rel_abund))\n\nWe can look at this data where we group by species.\n\nmetab_imputed_clustered_long_log10 %&gt;%\n  ggplot(aes(x = sample_name , y = rel_abund, fill = tomato)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(title = \"LC-MS (+) Feature Abundances by Sample\",\n       subtitle = \"Data is log10 transformed\",\n       y = \"Relative abundance\")\n\n\n\n\nWe can also look at this data by run order to see if we have any overall run order effects visible.\n\nmetab_imputed_clustered_long_log10 %&gt;%\n  mutate(sample_name = fct_reorder(sample_name, run_order)) %&gt;%\n  ggplot(aes(x = sample_name , y = rel_abund, fill = tomato)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(title = \"LC-MS (+) Feature Abundances by Sample\",\n       subtitle = \"Data is log10 transformed\",\n       y = \"Relative abundance\")\n\n\n\n\n\n\nPareto scaled\nPareto scaling scales but keeps the fidelity of the original differences in absolute measurement value more than autoscaling. Often data is Pareto scaled after log transofmration\n\nmetab_wide_meta_imputed_log2 &lt;- metab_imputed_clustered_long_log2 %&gt;%\n  select(-mz, -rt) %&gt;%\n  pivot_wider(names_from = \"mz_rt\",\n              values_from = \"rel_abund\")\n\nmetab_imputed_clustered_wide_log2_metabs &lt;- \n  metab_wide_meta_imputed_log2[,6:ncol(metab_wide_meta_imputed_log2)]\n\npareto_scaled &lt;- \n  IMIFA::pareto_scale(metab_imputed_clustered_wide_log2_metabs, center = FALSE)\n\npareto_scaled &lt;- bind_cols(metab_wide_meta_imputed_log2[,1:6], pareto_scaled)\n\nNew names:\n‚Ä¢ `104.059264569687_0.6302757` -&gt; `104.059264569687_0.6302757...6`\n‚Ä¢ `104.059264569687_0.6302757` -&gt; `104.059264569687_0.6302757...7`\n\n\n\npareto_scaled_long &lt;- pareto_scaled %&gt;%\n  pivot_longer(cols = 6:ncol(.),\n               names_to = \"mz_rt\",\n               values_to = \"rel_abund\")\n\npareto_scaled_long %&gt;%\n  # mutate(short_sample_name = fct_reorder(short_sample_name, treatment)) %&gt;%\n  ggplot(aes(x = sample_name, y = rel_abund, fill = tomato)) +\n  geom_boxplot(alpha = 0.6) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(title = \"LC-MS (+) Feature Abundances by Sample\",\n       subtitle = \"Data is Pareto scaled\",\n       y = \"Relative abundance\")\n\n\n\n\nI think pareto scaling is making everything look super the same. I am going to use log2 transformed data for the rest of this analysis."
  },
  {
    "objectID": "activities/05_R.html#pcas",
    "href": "activities/05_R.html#pcas",
    "title": "Data analysis with R",
    "section": "PCAs",
    "text": "PCAs\n\nWith QCs\n\npca_qc &lt;- prcomp(metab_wide_meta_imputed_log2[,-c(1:5)], # remove metadata\n                 scale = FALSE, # we did our own scaling\n                 center = TRUE) # true is the default\n\nsummary(pca_qc)\n\nImportance of components:\n                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     31.5978 13.9066 9.93235 8.13835 6.00911 5.27438 4.89711\nProportion of Variance  0.6171  0.1195 0.06098 0.04094 0.02232 0.01719 0.01482\nCumulative Proportion   0.6171  0.7367 0.79762 0.83856 0.86088 0.87807 0.89289\n                           PC8     PC9    PC10    PC11    PC12   PC13    PC14\nStandard deviation     4.15326 3.77844 3.63247 3.37842 3.25481 2.9004 2.79743\nProportion of Variance 0.01066 0.00882 0.00816 0.00705 0.00655 0.0052 0.00484\nCumulative Proportion  0.90356 0.91238 0.92054 0.92759 0.93414 0.9393 0.94417\n                          PC15    PC16    PC17    PC18    PC19   PC20    PC21\nStandard deviation     2.60922 2.50876 2.47274 2.33790 2.28918 2.2393 2.12246\nProportion of Variance 0.00421 0.00389 0.00378 0.00338 0.00324 0.0031 0.00278\nCumulative Proportion  0.94838 0.95227 0.95605 0.95943 0.96267 0.9658 0.96855\n                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     2.11490 2.06807 2.02501 1.95844 1.86445 1.82459 1.76587\nProportion of Variance 0.00276 0.00264 0.00253 0.00237 0.00215 0.00206 0.00193\nCumulative Proportion  0.97132 0.97396 0.97650 0.97887 0.98102 0.98307 0.98500\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     1.74077 1.67539 1.67410 1.65514 1.58903 1.53161 1.45988\nProportion of Variance 0.00187 0.00173 0.00173 0.00169 0.00156 0.00145 0.00132\nCumulative Proportion  0.98687 0.98861 0.99034 0.99203 0.99359 0.99504 0.99636\n                          PC36    PC37   PC38    PC39    PC40   PC41     PC42\nStandard deviation     1.41771 1.01189 0.9033 0.84205 0.82590 0.8036 2.29e-14\nProportion of Variance 0.00124 0.00063 0.0005 0.00044 0.00042 0.0004 0.00e+00\nCumulative Proportion  0.99760 0.99824 0.9987 0.99918 0.99960 1.0000 1.00e+00\n\n\nLook at how much variance is explained by each PC.\n\nimportance_qc &lt;- summary(pca_qc)$importance %&gt;%\n  as.data.frame()\n\nhead(importance_qc)\n\n                            PC1      PC2      PC3      PC4      PC5      PC6\nStandard deviation     31.59782 13.90658 9.932345 8.138347 6.009109 5.274385\nProportion of Variance  0.61711  0.11953 0.060980 0.040940 0.022320 0.017190\nCumulative Proportion   0.61711  0.73665 0.797620 0.838560 0.860880 0.878070\n                            PC7      PC8      PC9     PC10     PC11     PC12\nStandard deviation     4.897111 4.153257 3.778439 3.632472 3.378419 3.254811\nProportion of Variance 0.014820 0.010660 0.008820 0.008160 0.007050 0.006550\nCumulative Proportion  0.892890 0.903560 0.912380 0.920540 0.927590 0.934140\n                           PC13     PC14     PC15     PC16     PC17     PC18\nStandard deviation     2.900409 2.797427 2.609218 2.508758 2.472735 2.337897\nProportion of Variance 0.005200 0.004840 0.004210 0.003890 0.003780 0.003380\nCumulative Proportion  0.939340 0.944170 0.948380 0.952270 0.956050 0.959430\n                          PC19    PC20     PC21     PC22     PC23     PC24\nStandard deviation     2.28918 2.23927 2.122464 2.114901 2.068071 2.025009\nProportion of Variance 0.00324 0.00310 0.002780 0.002760 0.002640 0.002530\nCumulative Proportion  0.96267 0.96577 0.968550 0.971320 0.973960 0.976500\n                           PC25     PC26     PC27     PC28     PC29     PC30\nStandard deviation     1.958436 1.864446 1.824592 1.765867 1.740772 1.675393\nProportion of Variance 0.002370 0.002150 0.002060 0.001930 0.001870 0.001730\nCumulative Proportion  0.978870 0.981020 0.983070 0.985000 0.986870 0.988610\n                           PC31     PC32     PC33     PC34     PC35     PC36\nStandard deviation     1.674095 1.655141 1.589032 1.531608 1.459883 1.417712\nProportion of Variance 0.001730 0.001690 0.001560 0.001450 0.001320 0.001240\nCumulative Proportion  0.990340 0.992030 0.993590 0.995040 0.996360 0.997600\n                           PC37      PC38      PC39      PC40      PC41\nStandard deviation     1.011887 0.9032848 0.8420541 0.8258958 0.8035735\nProportion of Variance 0.000630 0.0005000 0.0004400 0.0004200 0.0004000\nCumulative Proportion  0.998240 0.9987400 0.9991800 0.9996000 1.0000000\n                               PC42\nStandard deviation     2.289946e-14\nProportion of Variance 0.000000e+00\nCumulative Proportion  1.000000e+00\n\n\nGenerate a scree plot.\n\nfviz_eig(pca_qc)\n\n\n\n\nGenerate a scores plot (points are samples) quickly with fviz_pca_ind.\n\nfviz_pca_ind(pca_qc)\n\n\n\n\nMake a scores plot but prettier.\n\n# create a df of pca_qc$x\nscores_raw_qc &lt;- as.data.frame(pca_qc$x)\n\n# bind meta-data\nscores_qc &lt;- bind_cols(metab_wide_meta_imputed_log2[,1:5], # first 5 columns\n                       scores_raw_qc)\n\nPlot.\n\n# create objects indicating percent variance explained by PC1 and PC2\nPC1_percent_qc &lt;- round((importance_qc[2,1])*100, # index 2nd row, 1st column, times 100\n                         1) # round to 1 decimal\nPC2_percent_qc &lt;- round((importance_qc[2,2])*100, 1) \n\n# plot\n# aes(text) is for setting tooltip with plotly later to indicate hover text\n(scores_qc_plot &lt;- scores_qc %&gt;%\n  ggplot(aes(x = PC1, y = PC2, fill = tomato, text = glue(\"Sample: {sample_name},\n                                                           Treatment: {tomato}\"))) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent_qc}%\"), \n       y = glue(\"PC2: {PC2_percent_qc}%\"), \n       title = \"PCA Scores Plot by Tomato Type\"))\n\n\n\n\nThen make your scores plot ineractive so you can see who is who.\n\nggplotly(scores_qc_plot, tooltip = \"text\")\n\n\n\n\n\nMake a loadings plot (points are features) even though it might not be that useful.\n\nfviz_pca_var(pca_qc)\n\n\n\n\nSee what I mean? Not that useful. There are some functions in PCAtools that label only the points that most contribute to each PC. Could also do this manually if its of interest.\n\n\nWithout QCs\n\nmetab_wide_meta_imputed_log2_noqc &lt;- metab_wide_meta_imputed_log2 %&gt;%\n  filter(sample_or_qc == \"Sample\")\n\n\npca_noqc &lt;- prcomp(metab_wide_meta_imputed_log2_noqc[,-c(1:5)], # remove metadata\n                 scale = FALSE, # we did our own scaling\n                 center = TRUE) # true is the default\n\nsummary(pca_noqc)\n\nImportance of components:\n                           PC1     PC2      PC3     PC4     PC5     PC6     PC7\nStandard deviation     33.8130 14.8180 10.35429 7.37824 5.87947 5.66811 4.56144\nProportion of Variance  0.6381  0.1225  0.05983 0.03038 0.01929 0.01793 0.01161\nCumulative Proportion   0.6381  0.7606  0.82044 0.85082 0.87011 0.88804 0.89965\n                           PC8     PC9    PC10    PC11    PC12    PC13   PC14\nStandard deviation     4.19510 3.94170 3.65660 3.52261 3.15233 3.02541 2.8696\nProportion of Variance 0.00982 0.00867 0.00746 0.00693 0.00555 0.00511 0.0046\nCumulative Proportion  0.90947 0.91815 0.92561 0.93253 0.93808 0.94319 0.9478\n                          PC15   PC16    PC17    PC18    PC19    PC20   PC21\nStandard deviation     2.72814 2.6756 2.53288 2.49238 2.42314 2.30610 2.2782\nProportion of Variance 0.00415 0.0040 0.00358 0.00347 0.00328 0.00297 0.0029\nCumulative Proportion  0.95194 0.9559 0.95951 0.96298 0.96626 0.96922 0.9721\n                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     2.22947 2.21499 2.12382 2.02431 1.99311 1.91079 1.88658\nProportion of Variance 0.00277 0.00274 0.00252 0.00229 0.00222 0.00204 0.00199\nCumulative Proportion  0.97489 0.97763 0.98015 0.98244 0.98465 0.98669 0.98868\n                          PC29    PC30   PC31    PC32    PC33    PC34    PC35\nStandard deviation     1.81478 1.80964 1.7951 1.71348 1.65351 1.57592 1.53143\nProportion of Variance 0.00184 0.00183 0.0018 0.00164 0.00153 0.00139 0.00131\nCumulative Proportion  0.99051 0.99234 0.9941 0.99578 0.99731 0.99869 1.00000\n                            PC36\nStandard deviation     2.318e-14\nProportion of Variance 0.000e+00\nCumulative Proportion  1.000e+00\n\n\nLook at how much variance is explained by each PC.\n\nimportance_noqc &lt;- summary(pca_noqc)$importance %&gt;%\n  as.data.frame()\n\nhead(importance_noqc)\n\n                            PC1      PC2      PC3      PC4      PC5      PC6\nStandard deviation     33.81302 14.81805 10.35429 7.378238 5.879475 5.668112\nProportion of Variance  0.63807  0.12254  0.05983 0.030380 0.019290 0.017930\nCumulative Proportion   0.63807  0.76061  0.82044 0.850820 0.870110 0.888040\n                            PC7      PC8      PC9     PC10     PC11     PC12\nStandard deviation     4.561444 4.195102 3.941697 3.656604 3.522607 3.152326\nProportion of Variance 0.011610 0.009820 0.008670 0.007460 0.006930 0.005550\nCumulative Proportion  0.899650 0.909470 0.918150 0.925610 0.932530 0.938080\n                           PC13     PC14     PC15     PC16     PC17     PC18\nStandard deviation     3.025414 2.869611 2.728145 2.675604 2.532883 2.492383\nProportion of Variance 0.005110 0.004600 0.004150 0.004000 0.003580 0.003470\nCumulative Proportion  0.943190 0.947780 0.951940 0.955930 0.959510 0.962980\n                           PC19     PC20     PC21     PC22     PC23     PC24\nStandard deviation     2.423142 2.306101 2.278208 2.229472 2.214987 2.123819\nProportion of Variance 0.003280 0.002970 0.002900 0.002770 0.002740 0.002520\nCumulative Proportion  0.966260 0.969220 0.972120 0.974890 0.977630 0.980150\n                           PC25     PC26     PC27     PC28    PC29     PC30\nStandard deviation     2.024309 1.993108 1.910792 1.886578 1.81478 1.809644\nProportion of Variance 0.002290 0.002220 0.002040 0.001990 0.00184 0.001830\nCumulative Proportion  0.982440 0.984650 0.986690 0.988680 0.99051 0.992340\n                           PC31     PC32     PC33    PC34     PC35      PC36\nStandard deviation     1.795088 1.713483 1.653513 1.57592 1.531426 2.318e-14\nProportion of Variance 0.001800 0.001640 0.001530 0.00139 0.001310 0.000e+00\nCumulative Proportion  0.994140 0.995780 0.997310 0.99869 1.000000 1.000e+00\n\n\nGenerate a scree plot.\n\nfviz_eig(pca_noqc)\n\n\n\n\nGenerate a scores plot (points are samples) quickly with fviz_pca_ind.\n\nfviz_pca_ind(pca_noqc)\n\n\n\n\nMake a scores plot but prettier.\n\n# create a df of pca_qc$x\nscores_raw_noqc &lt;- as.data.frame(pca_noqc$x)\n\n# bind meta-data\nscores_noqc &lt;- bind_cols(metab_wide_meta_imputed_log2_noqc[,1:5], # metadata\n                         scores_raw_noqc)\n\nPlot.\n\n# create objects indicating percent variance explained by PC1 and PC2\nPC1_percent_noqc &lt;- round((importance_noqc[2,1])*100, # index 2nd row, 1st column, times 100\n                         1) # round to 1 decimal\nPC2_percent_noqc &lt;- round((importance_noqc[2,2])*100, 1) \n\n# plot\n# aes(text) is for setting tooltip with plotly later to indicate hover text\n(scores_noqc_plot &lt;- scores_noqc %&gt;%\n  ggplot(aes(x = PC1, y = PC2, fill = tomato, text = glue(\"Sample: {sample_name},\n                                                             Treatment: {tomato}\"))) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent_noqc}%\"), \n       y = glue(\"PC2: {PC2_percent_noqc}%\"), \n       title = \"PCA Scores Plot Colored by Tomato Type\"))\n\n\n\n\nThen make your scores plot ineractive so you can see who is who.\n\nggplotly(scores_noqc_plot, tooltip = \"text\")\n\n\n\n\n\nMake a loadings plot (points are features) even though it might not be that useful.\n\nfviz_pca_var(pca_noqc)\n\n\n\n\nSee what I mean? Not that useful. There are some functions in PCAtools that label only the points that most contribute to each PC. Could also do this manually if its of interest."
  },
  {
    "objectID": "activities/05_R.html#univariate-testing",
    "href": "activities/05_R.html#univariate-testing",
    "title": "Data analysis with R",
    "section": "Univariate Testing",
    "text": "Univariate Testing\nI am going to include some sample univariate testing for comparisons between two and more than two groups.\n\nANOVA - &gt; 2 groups\nJust to test it out, I‚Äôm going to test for significant differences between our three tomato groups.\n\n# run series of t-tests\nanova &lt;- metab_imputed_clustered_long_log2 %&gt;%\n  filter(!tomato == \"QC\") %&gt;% # remove QCs\n  dplyr::select(sample_name, tomato, mz_rt, rel_abund) %&gt;%\n  group_by(mz_rt) %&gt;%\n  anova_test(rel_abund ~ tomato)\n\n# adjust pvalues for multiple testing\nanova_padjusted &lt;- p.adjust(anova$p, method = \"BH\") %&gt;%\n  as.data.frame() %&gt;%\n  rename(p_adj = 1)\n\nanova_padj &lt;- bind_cols(as.data.frame(anova), anova_padjusted) %&gt;%\n  rename(padj = 9)\n\n# extract out only the significantly different features\nanova_sig &lt;- anova_padj %&gt;%\n  as.data.frame() %&gt;%\n  filter(p &lt;= 0.05)\n\n# how many features are significantly different between the groups?\nnrow(anova_sig)\n\n[1] 1156\n\n\nDo we think this is reasonable? What if I make a quick boxplot of the relative intensity of the feature that has the smallest p-value from this comparison (236.200891807883_6.022641).\n\nmetab_imputed_clustered_long_log2 %&gt;%\n  filter(mz_rt == \"236.200891807883_6.022641\") %&gt;%\n  ggplot(aes(x = tomato, y = rel_abund, color = tomato)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sample type\",\n       y = \"Log2 relative abundance of 236.200891807883_6.022641\",\n       title = \"Difference in log2 relative abundance of 236.200891807883_6.022641 \\nbetween tomato types\")\n\n\n\n\nOk we can see why this is very different across the groups :)\n\n\nT-test, 2 groups\n\nNon-parametric\nData might not be normally distributed so I did a nonparametric test.\n\n# run series of t-tests\noh8243_hats_nonparam &lt;- metab_imputed_clustered_long_log2 %&gt;%\n  filter(tomato == \"OH8243\" | tomato == \"HATS\") %&gt;%\n  dplyr::select(sample_name, tomato, mz_rt, rel_abund) %&gt;%\n  group_by(mz_rt) %&gt;%\n  wilcox_test(rel_abund ~ tomato, \n         paired = FALSE, \n         detailed = TRUE, # gives you more detail in output\n         p.adjust.method = \"BH\") %&gt;% # Benjamini-Hochberg false discovery rate multiple testing correction\n  add_significance() %&gt;%\n  arrange(p)\n\n# extract out only the significantly different features\noh8243_hats_nonparam_sig &lt;- oh8243_hats_nonparam %&gt;%\n  filter(p &lt;= 0.05)\n\n# how many features are significantly different between the groups?\nnrow(oh8243_hats_nonparam_sig)\n\n[1] 663\n\n\nLet‚Äôs do the same thing again here with a boxplot.\n\nmetab_imputed_clustered_long_log2 %&gt;%\n  filter(mz_rt == \"1004.54196498357_5.227879\") %&gt;%\n  filter(tomato == \"HATS\" | tomato == \"OH8243\") %&gt;%\n  ggplot(aes(x = tomato, y = rel_abund, color = tomato)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sample type\",\n       y = \"Log2 relative abundance of 1004.54196498357_5.227879\",\n       title = \"Difference in log2 relative abundance of 1004.54196498357_5.227879 \\nbetween OH8243 and HATS\")\n\n\n\n\nWrite out the significantly different features.\n\nwrite_csv(oh8243_hats_nonparam_sig,\n          file = \"data/oh8243_hats_nonparam_sig.csv\")\n\n\n\nParametric\nOr we can assume data are normally distributed\n\n# run series of t-tests\noh8243_hats_param &lt;- metab_imputed_clustered_long_log2 %&gt;%\n  filter(tomato == \"OH8243\" | tomato == \"HATS\") %&gt;%\n  dplyr::select(sample_name, tomato, mz_rt, rel_abund) %&gt;%\n  group_by(mz_rt) %&gt;%\n  t_test(rel_abund ~ tomato, \n         paired = FALSE, \n         detailed = TRUE, # gives you more detail in output\n         p.adjust.method = \"BH\") %&gt;% # Benjamini-Hochberg false discovery rate multiple testing correction\n  add_significance() %&gt;%\n  arrange(p)\n\n# extract out only the significantly different features\noh8243_hats_param_sig &lt;- oh8243_hats_param %&gt;%\n  filter(p &lt;= 0.05)\n\n# how many features are significantly different between the groups?\nnrow(oh8243_hats_param_sig)\n\n[1] 663\n\n\nLet‚Äôs do the same thing again here with a boxplot.\n\nmetab_imputed_clustered_long_log2 %&gt;%\n  filter(mz_rt == \"474.357920874411_5.0924287\") %&gt;%\n  filter(tomato == \"HATS\" | tomato == \"OH8243\") %&gt;%\n  ggplot(aes(x = tomato, y = rel_abund, color = tomato)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sample type\",\n       y = \"Log2 relative abundance of 474.357920874411_5.0924287\",\n       title = \"Difference in log2 relative abundance of 474.357920874411_5.0924287 \\nbetween OH8243 and HATS\")\n\n\n\nplot &lt;- metab_imputed_clustered_long_log2 %&gt;%\n  filter(mz_rt == \"474.357920874411_5.0924287\") %&gt;%\n  filter(tomato == \"HATS\" | tomato == \"OH8243\") %&gt;%\n  ggplot(aes(x = tomato, y = rel_abund, color = tomato)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sample type\",\n       y = \"Log2 relative abundance of 474.3579_5.092\",\n       title = \"Difference in log2 relative abundance of 474.3579_5.092 \\nbetween OH8243 and HATS\")\n\nWrite out the significantly different features.\n\nwrite_csv(oh8243_hats_nonparam_sig,\n          file = \"data/oh8243_hats_param_sig.csv\")\n\n\n\nVolcano plot\nLet‚Äôs make a volcano plot so we can see which features are significantly different between OH8243 and HATS.\nFirst we wrangle.\n\noh8243_hats_FC &lt;- metab_imputed_clustered_long_log2 %&gt;%\n  filter(tomato == \"OH8243\" | tomato == \"HATS\") %&gt;%\n  group_by(tomato, mz_rt) %&gt;%\n  summarize(mean = mean(rel_abund)) %&gt;%\n  pivot_wider(names_from = tomato, values_from = mean) %&gt;%\n  mutate(HATS_minus_OH8243_log2FC = (HATS - OH8243))\n\n`summarise()` has grouped output by 'tomato'. You can override using the\n`.groups` argument.\n\noh8243_hats_FC_pval &lt;- left_join(oh8243_hats_FC, oh8243_hats_param, by = \"mz_rt\") %&gt;%\n  mutate(neglog10p = -log10(p))\n\nhead(oh8243_hats_FC_pval)\n\n# A tibble: 6 √ó 21\n  mz_rt    HATS OH8243 HATS_minus_OH8243_lo‚Ä¶¬π estimate estimate1 estimate2 .y.  \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;                  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n1 1002.1‚Ä¶  12.7  12.8                 -0.119   -0.119       12.7     12.8  rel_‚Ä¶\n2 1004.5‚Ä¶  14.6   9.56                 5.07     5.07        14.6      9.56 rel_‚Ä¶\n3 1007.2‚Ä¶  11.3  11.7                 -0.428   -0.428       11.3     11.7  rel_‚Ä¶\n4 1007.5‚Ä¶  11.5  12.1                 -0.592   -0.592       11.5     12.1  rel_‚Ä¶\n5 1010.4‚Ä¶  12.4  12.3                  0.0519   0.0519      12.4     12.3  rel_‚Ä¶\n6 1010.7‚Ä¶  12.9  12.4                  0.514    0.514       12.9     12.4  rel_‚Ä¶\n# ‚Ñπ abbreviated name: ¬π‚ÄãHATS_minus_OH8243_log2FC\n# ‚Ñπ 13 more variables: group1 &lt;chr&gt;, group2 &lt;chr&gt;, n1 &lt;int&gt;, n2 &lt;int&gt;,\n#   statistic &lt;dbl&gt;, p &lt;dbl&gt;, df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;,\n#   method &lt;chr&gt;, alternative &lt;chr&gt;, p.signif &lt;chr&gt;, neglog10p &lt;dbl&gt;\n\n\nLooking at features that are at least 2 fold change between groups and significantly different at p&lt;0.05.\n\nhigher_HATS &lt;- oh8243_hats_FC_pval %&gt;%\n  filter(neglog10p &gt;= 1.3 & HATS_minus_OH8243_log2FC &gt;= 1)\n\nhigher_OH8243 &lt;- oh8243_hats_FC_pval %&gt;%\n  filter(neglog10p &gt;= 1.3 & HATS_minus_OH8243_log2FC &lt;= -1)\n\n(oh8243_hats_volcano &lt;- oh8243_hats_FC_pval %&gt;%\n  ggplot(aes(x = HATS_minus_OH8243_log2FC, y = neglog10p, text = mz_rt)) +\n  geom_point() +\n  geom_point(data = higher_HATS, \n             aes(x = HATS_minus_OH8243_log2FC, y = neglog10p),\n             color = \"red\") +\n  geom_point(data = higher_OH8243, \n             aes(x = HATS_minus_OH8243_log2FC, y = neglog10p),\n             color = \"blue\") +\n  geom_hline(yintercept = 1.3, linetype = \"dashed\") +\n  geom_vline(xintercept = -1, linetype = \"dashed\") +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  labs(x = \"-log2 fold change\",\n       y = \"-log10 p-value\",\n       title = \"OH8243 vs HATS, positive mode\",\n       subtitle = \"Higher in HATS on top right (red), higher in OH8243 on top left (blue)\",\n       caption = \"Dotted lines represent a 2 fold difference between groups and a p-value &lt; 0.05\"))\n\n\n\n\nMake the plot interactive.\n\nggplotly(oh8243_hats_volcano, tooltip = \"text\")"
  },
  {
    "objectID": "activities/05_R.html#k-means-clustering",
    "href": "activities/05_R.html#k-means-clustering",
    "title": "Data analysis with R",
    "section": "K-means clustering",
    "text": "K-means clustering\nConduct k-means clustering on our data to see if we do have a natural 3 groups here.\nFirst I am wrangling the data by removing metadata.\n\nfor_kmeans &lt;- metab_wide_meta_imputed_log2 %&gt;%\n  filter(!sample_or_qc == \"QC\") %&gt;%\n  select(-sample_name, -sample_or_qc, -tomato, -rep_or_plot, -run_order)\n\nThen I can calculate within cluster sum of square errors.\n\n# calculate within cluster sum of squared errors wss\nwss &lt;- vector()\nfor (i in 1:10) {\n  tomato_kmeans &lt;- kmeans(for_kmeans, centers = i, nstart = 20)\n  wss[i] &lt;- tomato_kmeans$tot.withinss\n}\n\nFollowed by a scree plot which helps us see how many clusters we might have in our data.\n\nplot(1:10, wss, type = \"b\", \n     xlab = \"Number of Clusters\", \n     ylab = \"Within groups sum of squares\")\n\n\n\n\nTo me, I might pick the elbow at 3 clusters. You could change this or try different numbers of clusters and see how that affects your results.\n\n# set number of clusters to be 3\nk &lt;- 3\n\n\nkmeans_3 &lt;- kmeans(for_kmeans, \n                   centers = k, \n                   nstart = 20, \n                   iter.max = 200)\n\nsummary(kmeans_3)\n\n             Length Class  Mode   \ncluster        36   -none- numeric\ncenters      3834   -none- numeric\ntotss           1   -none- numeric\nwithinss        3   -none- numeric\ntot.withinss    1   -none- numeric\nbetweenss       1   -none- numeric\nsize            3   -none- numeric\niter            1   -none- numeric\nifault          1   -none- numeric\n\n\nWhich samples are in which cluster?\n\nkmeans_3$cluster\n\n [1] 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 2 2 2 2 2 2 2\n\n\nNow we can add the cluster identity to our metadata so we can visualizate our data based on the kmeans clustering.\n\n# Add the cluster group to the parent datafile\nscores_noqc_kmeans &lt;- scores_noqc %&gt;%\n  mutate(kmeans_3 = kmeans_3$cluster)\n\n# reorder so kmeans cluster is towards the beginning\nscores_noqc_kmeans &lt;- scores_noqc_kmeans %&gt;%\n  select(sample_name, sample_or_qc, tomato, rep_or_plot, run_order, kmeans_3, everything())\n\n# check the reordering\nknitr::kable(scores_noqc_kmeans[, 1:7])\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_name\nsample_or_qc\ntomato\nrep_or_plot\nrun_order\nkmeans_3\nPC1\n\n\n\n\nHATS_402_041\nSample\nHATS\n402\n41\n3\n-23.24793\n\n\nHATS_403_037\nSample\nHATS\n403\n37\n3\n-17.01461\n\n\nHATS_404_040\nSample\nHATS\n404\n40\n3\n-21.49241\n\n\nHATS_401_022\nSample\nHATS\n401\n22\n3\n-16.35241\n\n\nHATS_408_043\nSample\nHATS\n408\n43\n3\n-15.27909\n\n\nHATS_405_025\nSample\nHATS\n405\n25\n3\n-20.42581\n\n\nHATS_406_055\nSample\nHATS\n406\n55\n3\n-20.87974\n\n\nHATS_407_059\nSample\nHATS\n407\n59\n3\n-21.20251\n\n\nHATS_412_019\nSample\nHATS\n412\n19\n3\n-14.14882\n\n\nHATS_409_056\nSample\nHATS\n409\n56\n3\n-15.66016\n\n\nHATS_410_018\nSample\nHATS\n410\n18\n3\n-15.15110\n\n\nHATS_411_044\nSample\nHATS\n411\n44\n3\n-15.69849\n\n\nLA2213_602_035\nSample\nLA2213\n602\n35\n1\n41.19750\n\n\nLA2213_603_045\nSample\nLA2213\n603\n45\n1\n49.19390\n\n\nLA2213_601_051\nSample\nLA2213\n601\n51\n1\n49.69095\n\n\nLA2213_604_016\nSample\nLA2213\n604\n16\n1\n46.56555\n\n\nLA2213_605_017\nSample\nLA2213\n605\n17\n1\n46.48514\n\n\nLA2213_607_032\nSample\nLA2213\n607\n32\n1\n45.99993\n\n\nLA2213_608_024\nSample\nLA2213\n608\n24\n1\n45.81353\n\n\nLA2213_606_033\nSample\nLA2213\n606\n33\n1\n49.70804\n\n\nLA2213_609_058\nSample\nLA2213\n609\n58\n1\n45.76014\n\n\nLA2213_610_050\nSample\nLA2213\n610\n50\n1\n43.55337\n\n\nLA2213_612_046\nSample\nLA2213\n612\n46\n1\n44.25618\n\n\nOH8243_801_036\nSample\nOH8243\n801\n36\n2\n-22.74986\n\n\nOH8243_802_026\nSample\nOH8243\n802\n26\n2\n-25.64428\n\n\nOH8243_803_030\nSample\nOH8243\n803\n30\n2\n-26.66404\n\n\nOH8243_805_057\nSample\nOH8243\n805\n57\n2\n-31.27910\n\n\nLA2213_611_061\nSample\nLA2213\n611\n61\n1\n50.50206\n\n\nOH8243_804_029\nSample\nOH8243\n804\n29\n2\n-30.49216\n\n\nOH8243_806_027\nSample\nOH8243\n806\n27\n2\n-36.03705\n\n\nOH8243_809_063\nSample\nOH8243\n809\n63\n2\n-29.82613\n\n\nOH8243_807_053\nSample\nOH8243\n807\n53\n2\n-28.82398\n\n\nOH8243_810_049\nSample\nOH8243\n810\n49\n2\n-26.33023\n\n\nOH8243_808_038\nSample\nOH8243\n808\n38\n2\n-26.40468\n\n\nOH8243_812_060\nSample\nOH8243\n812\n60\n2\n-31.83884\n\n\nOH8243_811_028\nSample\nOH8243\n811\n28\n2\n-26.08287\n\n\n\n\n\nNow we can color our PCA based on our kmeans clustering.\n\n(scores_noqc_kmeans_plot &lt;- scores_noqc_kmeans %&gt;%\n  ggplot(aes(x = PC1, y = PC2, fill = as.factor(kmeans_3), shape = tomato,\n               text = glue(\"Sample: {sample_name},\n                            Treatment: {tomato}\"))) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent_noqc}%\"), \n       y = glue(\"PC2: {PC2_percent_noqc}%\"), \n       title = \"PCA Scores Plot Colored by K-means Cluster\",\n       fill = \"K-means cluster\"))\n\n\n\n\nLooks like the K-means clusters are aligned exactly with our tomato type."
  },
  {
    "objectID": "activities/05_R.html#hierarchical-clustering",
    "href": "activities/05_R.html#hierarchical-clustering",
    "title": "Data analysis with R",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nConduct hierarchical clustering to see if our data naturally is clustered into our three tomato groups. Here I will calculate a distance matrix using Wards distance, or minimal increase in sum of squares (MISSQ), good for more cloud and spherical shaped groups, and running HCA\n\n# the df for kmeans also works for HCA but needs to be a matrix\nfor_hca &lt;- as.matrix(for_kmeans)\n\n# calculate a distance matrix\ndist_matrix &lt;- distance(for_hca)\n\nMetric: 'euclidean'; comparing: 36 vectors.\n\n# making the true distance matrix\ntrue_dist_matrix &lt;- as.dist(dist_matrix)\n\n# conduct HCA\nhclust_output &lt;- hclust(d = true_dist_matrix, method = \"ward.D\")\n\nsummary(hclust_output)\n\n            Length Class  Mode     \nmerge       70     -none- numeric  \nheight      35     -none- numeric  \norder       36     -none- numeric  \nlabels      36     -none- character\nmethod       1     -none- character\ncall         3     -none- call     \ndist.method  0     -none- NULL     \n\n\nNow we can plot:\n\nplot(hclust_output,\n     main = \"Hierarchical clustering of samples\")\n\n\n\n\nOur samples are not named, let‚Äôs bring back which sample is which.\n\n# grab metadata\nhca_meta &lt;- metab_wide_meta_imputed_log2 %&gt;%\n  filter(!sample_or_qc == \"QC\") %&gt;%\n  select(sample_name, sample_or_qc, tomato, rep_or_plot, run_order)\n\n# bind to hca object\nhclust_output$labels &lt;- hca_meta$sample_name\n\nNow we can plot:\n\nplot(hclust_output,\n     main = \"Hierarchical clustering of samples\")\n\n\n\n\nThis base R plotting is too ugly for me I gotta do something about it.\n\nggdendrogram(hclust_output, rotate = TRUE, theme_dendro = FALSE)\n\n\n\n\n\n# create a dendrogram object so we can plot better late\ndend &lt;- as.dendrogram(hclust_output)\n\ndend_data &lt;- dendro_data(dend, type = \"rectangle\")\nnames(dend_data)\n\n[1] \"segments\"    \"labels\"      \"leaf_labels\" \"class\"      \n\nhead(dend_data$segments)\n\n         x         y     xend      yend\n1 14.32812 734.90047  6.87500 734.90047\n2  6.87500 734.90047  6.87500  49.19104\n3 14.32812 734.90047 21.78125 734.90047\n4 21.78125 734.90047 21.78125 225.75525\n5  6.87500  49.19104  3.62500  49.19104\n6  3.62500  49.19104  3.62500  32.07677\n\nhead(dend_data$labels)\n\n  x y          label\n1 1 0 LA2213_608_024\n2 2 0 LA2213_609_058\n3 3 0 LA2213_610_050\n4 4 0 LA2213_606_033\n5 5 0 LA2213_611_061\n6 6 0 LA2213_604_016\n\n# create a new column called group so we can color by it\ndend_data$labels &lt;- dend_data$labels %&gt;%\n  separate_wider_delim(cols = label, \n                       delim = \"_\", \n                       names = c(\"group\", \"rep\", \"run_order\"),\n                       cols_remove = FALSE) %&gt;%\n  select(x, y, label, group)\n\nggplot(dend_data$segments) + \n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+\n  geom_text(data = dend_data$labels, aes(x, y, label = label, color = group),\n            hjust = 1, angle = 90, size = 3) +\n  ylim(-300, 800) +\n  labs(title = \"Hierarchical clustering of tomato samples\")"
  },
  {
    "objectID": "activities/05_R.html#pls-da",
    "href": "activities/05_R.html#pls-da",
    "title": "Data analysis with R",
    "section": "PLS-DA",
    "text": "PLS-DA\nPartial least squares discriminant analysis (or regression) is a method for classification/prediction of high dimensional data. Many people use PLS to find the features most distinguishing groups (or more associated with an outcome Y). Personally I don‚Äôt really like PLS analysis - I don‚Äôt find its any more useful that running a series of univariate tests to see which feautures are differentially abundant by group, and it has a bad tendency to overfit especially with noisy data (and one could argue metabolomics data is always noisy). Still, I‚Äôll show you how to do it.\nAs we went over in the lecture material, PLS based methods are prediction methods though most use them in metabolomics to discover features of interest.\n\nfor_PLS &lt;- metab_wide_meta_imputed_log2 %&gt;%\n  select(-sample_name, -sample_or_qc, -tomato, -rep_or_plot, -run_order) # remove meta\n\npls &lt;- opls(x = for_PLS, y = metab_wide_meta_imputed_log2$tomato,\n            orthoI = 0) # for PLS\n\nPLS-DA\n42 samples x 1278 variables and 1 response\nstandard scaling of predictors and response(s)\n      R2X(cum) R2Y(cum) Q2(cum)  RMSEE pre ort pR2Y  pQ2\nTotal    0.727    0.986   0.964 0.0528   5   0 0.05 0.05"
  },
  {
    "objectID": "activities/05_R.html#random-forest",
    "href": "activities/05_R.html#random-forest",
    "title": "Data analysis with R",
    "section": "Random forest",
    "text": "Random forest"
  },
  {
    "objectID": "activities/03_mzmine.html",
    "href": "activities/03_mzmine.html",
    "title": "Spectral pre-processing with MZmine",
    "section": "",
    "text": "Now that we‚Äôve gone through the collection of LC-MS metabolomics data, we will take our raw datafiles and convert them into a feature table.\n\n\nWe are going to be using some sample data coming from the my lab. These samples are methanolic extracts (more details about their creation are described in Dzakovich et al., Plant Genome 2022) of tomatoes with 3 different genotypes:\n\nOH8243: a typical processing tomato\nLA2213: a wild currant tomato that is high in steroidal alkaloids\nHATS: a high alpha tomatine selection (HATS) that came from a breeding scheme crossing the previous two tomatoes and selecting fruits high in steroidal alkaloids.\n\nAdditionally, we have quality control (QC) samples that are made from pools of each of the tomato extracts, and process blanks (PB) which underwent the same extraction process but instead of adding tomato, added an equivalent mass of water.\nThe extracts were analyzed using a C18 column in positive ion mode on an Agilent 6546 QTOF-MS. The gradient portion of our method is from 0.5 - 7.5 min.\nYou can find a link here to a OneDrive folder that contains all the raw data files to download.\nSamples are named with the following structure:\n\nFirst term is the type of sample (OH8243, LA2213, HATS, QC or PB)\nThe second 3 digit number starting with 4XX, 6XX, or 8XX is a plot code. The QCs and PBs have a two digit number which represents tells you which replicate it is.\nThe last number is the run order\n\nI have not given you every sample from this run, so the run order numbers won‚Äôt be continuous. There are 12 of each of the tomato varieties, 6 QCs, and 3 PBs. If your MZmine is getting bogged down, you don‚Äôt need to use all of these samples, you can practice in using just the QCs. I will give you full feature tables for the data analysis next steps.\nDownload the raw data files and put them in a known location on your computer."
  },
  {
    "objectID": "activities/03_mzmine.html#our-data",
    "href": "activities/03_mzmine.html#our-data",
    "title": "Spectral pre-processing with MZmine",
    "section": "",
    "text": "We are going to be using some sample data coming from the my lab. These samples are methanolic extracts (more details about their creation are described in Dzakovich et al., Plant Genome 2022) of tomatoes with 3 different genotypes:\n\nOH8243: a typical processing tomato\nLA2213: a wild currant tomato that is high in steroidal alkaloids\nHATS: a high alpha tomatine selection (HATS) that came from a breeding scheme crossing the previous two tomatoes and selecting fruits high in steroidal alkaloids.\n\nAdditionally, we have quality control (QC) samples that are made from pools of each of the tomato extracts, and process blanks (PB) which underwent the same extraction process but instead of adding tomato, added an equivalent mass of water.\nThe extracts were analyzed using a C18 column in positive ion mode on an Agilent 6546 QTOF-MS. The gradient portion of our method is from 0.5 - 7.5 min.\nYou can find a link here to a OneDrive folder that contains all the raw data files to download.\nSamples are named with the following structure:\n\nFirst term is the type of sample (OH8243, LA2213, HATS, QC or PB)\nThe second 3 digit number starting with 4XX, 6XX, or 8XX is a plot code. The QCs and PBs have a two digit number which represents tells you which replicate it is.\nThe last number is the run order\n\nI have not given you every sample from this run, so the run order numbers won‚Äôt be continuous. There are 12 of each of the tomato varieties, 6 QCs, and 3 PBs. If your MZmine is getting bogged down, you don‚Äôt need to use all of these samples, you can practice in using just the QCs. I will give you full feature tables for the data analysis next steps.\nDownload the raw data files and put them in a known location on your computer."
  },
  {
    "objectID": "activities/03_mzmine.html#download-msconvert-from-proteowizard",
    "href": "activities/03_mzmine.html#download-msconvert-from-proteowizard",
    "title": "Spectral pre-processing with MZmine",
    "section": "2.1 Download MSConvert from ProteoWizard",
    "text": "2.1 Download MSConvert from ProteoWizard\nIf you don‚Äôt have ProteoWizard, you can download it here: https://proteowizard.sourceforge.io/. Once you download the program, open MSConvert."
  },
  {
    "objectID": "activities/03_mzmine.html#select-files-for-conversion",
    "href": "activities/03_mzmine.html#select-files-for-conversion",
    "title": "Spectral pre-processing with MZmine",
    "section": "2.2 Select files for conversion",
    "text": "2.2 Select files for conversion\n\n\n\nSelecting files to convert in MSConvert\n\n\n\nOpen up MSConvert and in the top left corner of the program, select the files you want to convert. Your directories are going to be different than mine depending on where your files are stored.\nSelect where on your computer you want the converted files to go, and indicate that location in Output Directory."
  },
  {
    "objectID": "activities/03_mzmine.html#set-parameters-for-conversion",
    "href": "activities/03_mzmine.html#set-parameters-for-conversion",
    "title": "Spectral pre-processing with MZmine",
    "section": "2.3 Set parameters for conversion",
    "text": "2.3 Set parameters for conversion\n\n\n\nSetting parameters for conversion in MSConvert\n\n\n\nUncheck zlib compression\nWe are going to convert our profile into centroided data which I think makes the deconvolution in MZmine easier. We will do that by under Filter, selecting Peak Picking, and picking the Vendor option under Algorithm. Here, we only have MS1 data so we only want MS level 1. We will add this to the table below.\nThe more files you have, the longer this will take, you can indicate that convert more than 1 file in parallel. Doing 4 at a time, about 40 samples took my lab data processing computer (which has a 128 MB RAM and 16 cores) a couple of minutes."
  },
  {
    "objectID": "activities/03_mzmine.html#using-mzmine-at-osc-ohio-supercomputer-center",
    "href": "activities/03_mzmine.html#using-mzmine-at-osc-ohio-supercomputer-center",
    "title": "Spectral pre-processing with MZmine",
    "section": "3.1 Using MZmine at OSC (Ohio Supercomputer Center)",
    "text": "3.1 Using MZmine at OSC (Ohio Supercomputer Center)\nSome parts of the MZmine pre-processing can be quite ram intensive and may struggle to run on your laptop. To solve this problem, you can run your deconvolution on a supercomputer. I‚Äôm providing here some instructions on how to do this at OSC.\n\n3.1.1 Create an OSC account\nYou can create an account at OSC at this link.\n\n\n3.1.2 Share your OSC username with Jess\nThis will allow Jess to add you to our OSC project which will direct the billing appropriately (to Jess, not to you).\n\n\n3.1.3 Log onto OSC\nNavigate to https://ondemand.osc.edu/ and log in. Make sure you are using PAS2568.\n\n\n3.1.4 Download the portable Linux version of MZmine\nMZmine has a portal Linux version which you can download while on OSC, and then use. Navigate to https://ondemand.osc.edu/. You can open a Pitzer desktop session.\n\n\n\nAccessing Pitzer on OSC\n\n\nYou can use the parameters below for launching the Pitzer desktop. Make sure you select project PAS2568. You can adjust how many hours you need based on what time you have. Then click ‚ÄúLaunch‚Äù in blue at the bottom.\n\n\n\nSet parameters for launching a Pitzer desktop\n\n\n\nIf Pitzer is not working, you can open up an Owens desktop and it will work the same. Just use the same paramteres.\n\nOnce you are in Pitzer desktop, you can open an internet browser and navigate to https://github.com/mzmine/mzmine3/releases/tag/v3.9.0. At the time I put this tutorial together, the most recent version of MZmine is 3.9.0. Download the version for the Linux portable version.\nYou can actually skip the download process as I put the portable version fo MZmine 3.9.0 in /fs/ess/PAS2568/MZmine\n\n\n3.1.5 Open MZmine\nOnce you download MZmine, unzip the file, right click on the bin directory and select Open a terminal here.\nThen in your terminal, execute the following code one line at a time:\n\nchmod a+x MZmine\n\n\n./MZmine\n\nThe MZmine GUI should now be open."
  },
  {
    "objectID": "activities/03_mzmine.html#using-mzmine-on-your-own-computer",
    "href": "activities/03_mzmine.html#using-mzmine-on-your-own-computer",
    "title": "Spectral pre-processing with MZmine",
    "section": "3.2 Using MZmine on your own computer",
    "text": "3.2 Using MZmine on your own computer\nIf you don‚Äôt have MZmine, you can download it at this link: https://mzmine.github.io/.\nMZmine has extremely good documentation, so if you want to read more about any particular parameter, you can do so here: https://mzmine.github.io/mzmine_documentation/index.html."
  },
  {
    "objectID": "activities/03_mzmine.html#save-your-project",
    "href": "activities/03_mzmine.html#save-your-project",
    "title": "Spectral pre-processing with MZmine",
    "section": "4.1 Save your project",
    "text": "4.1 Save your project\nYou can save your project after different steps, this process can take a little while, but it allows you to go back to any part of your workflow and look at results or make changes.\nIn the top toolbar, navigate to Project &gt; Save project as and put your project files in an appropriate directory on your computer. A good place would be in the parent directory where your data files are.\nYou can save your file as Referencing (small) given that you don‚Äôt move where the input files are. This takes up less space but can lead to later incompatibility projects if files get moved. The status bar at the bottom of MZmine will tell you your current status.\n\n\n\nSaving your project in MZmine"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metabolomics short course, UFRGS üáßüá∑ May 2024",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D. Associate Professor, Horticulture and Crop Science, Food Science and Technology The Ohio State University\nThe purpose of this short course is to give students a conceptual and hands-on introduction to the metabolomics workflow, experimental design process, collection of data via LC combined with high resolution MS, data analysis process, and compound identification.\nBelow is an overview of the material to be covered in this workshop. The top toolbar has lecture material under the Lectures tab and Hands on activities under the Activities tab."
  },
  {
    "objectID": "index.html#introduction-to-metabolomics",
    "href": "index.html#introduction-to-metabolomics",
    "title": "Metabolomics short course, UFRGS üáßüá∑ May 2024",
    "section": "Introduction to metabolomics",
    "text": "Introduction to metabolomics"
  },
  {
    "objectID": "index.html#study-design-and-sample-collection",
    "href": "index.html#study-design-and-sample-collection",
    "title": "Metabolomics short course, UFRGS üáßüá∑ May 2024",
    "section": "Study design and sample collection",
    "text": "Study design and sample collection"
  },
  {
    "objectID": "index.html#lc-ms-data-acquisition-and-pre-processing",
    "href": "index.html#lc-ms-data-acquisition-and-pre-processing",
    "title": "Metabolomics short course, UFRGS üáßüá∑ May 2024",
    "section": "LC-MS data acquisition and pre-processing",
    "text": "LC-MS data acquisition and pre-processing\n\nBasics of LC and MS\nSample preparation\nData collection\nRaw data processing to create a feature table using MZmine"
  },
  {
    "objectID": "index.html#data-analysis-using-metaboanalyst",
    "href": "index.html#data-analysis-using-metaboanalyst",
    "title": "Metabolomics short course, UFRGS üáßüá∑ May 2024",
    "section": "Data analysis using MetaboAnalyst",
    "text": "Data analysis using MetaboAnalyst\n\nHow to use MetaboAnalyst\nLooking at your data and assessing quality\nCreating principal components analysis (PCA) plots\nUnivariate group difference testing\nMultivariate class discovery and prediction methods"
  },
  {
    "objectID": "index.html#compound-identification",
    "href": "index.html#compound-identification",
    "title": "Metabolomics short course, UFRGS üáßüá∑ May 2024",
    "section": "Compound identification",
    "text": "Compound identification"
  },
  {
    "objectID": "activities/04_metaboanalyst.html",
    "href": "activities/04_metaboanalyst.html",
    "title": "Data analysis with MetaboAnalyst",
    "section": "",
    "text": "Now that we have a filtered, final feature table, we can get started on our data analysis, including understanding if our data is of good quality before we begin to make biological interpretations as to what our results mean. We are going to conduct this analysis using MetaboAnalyst which is at version 6.0 when I put together this tutorial. You can read more about MetaboAnalyist on their website or in their publications (Ewald et al. 2024).\n\n\nBefore we upload our data, let‚Äôs make sure its in a suitable format. If we go to the Data Formats link on the left side of the website, we can see example data sets which will help us format our data. Our data looks like this right out of MZmine:\n\n\n\nOur data, straight from MZmine\n\n\n\n\n\n\n\n\nOur data contains samples in rows, and features in columns.\n\n\n\n\n\n\nWe will start with doing statistics (one factor) and we have a peak intensity table, so the file called lcms_table.csv is a good one for us to look at.\n\n\n\nLCMS sample data\n\n\nIn this data, we have features in rows and samples in columns. There is also a header row which contains the unique sample names, and a second row called Label which contains the groups. We will need to adjust our data to look like this. This includes\n\nAdding a row called Label which contains the sample groups (here, QC, HATS, LA2213, or OH8243).\n\n\n\n\nAdd a row which contains Label and the tomato identities\n\n\n\nCreating a single column with the mz_rt as a unique identifier. We can do this with the function =CONCATENATE() in Excel. Then we can fill down the row. Then be sure to copy this column and Paste &gt; Paste special &gt; Values since when we later remove some of the other columns that are inputs to this one, the formulas will break.\n\n\n\n\nHow to combine the row m/z and row retention time columns to be one unique mz_rt\n\n\n\nNow we can remove row ID, row m/z and row retention time.\n\nOur data should now look like this:\n\n\n\nOur data ready to import into MetaboAnalyst"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#data-formatting",
    "href": "activities/04_metaboanalyst.html#data-formatting",
    "title": "Data analysis with MetaboAnalyst",
    "section": "",
    "text": "Before we upload our data, let‚Äôs make sure its in a suitable format. If we go to the Data Formats link on the left side of the website, we can see example data sets which will help us format our data. Our data looks like this right out of MZmine:\n\n\n\nOur data, straight from MZmine\n\n\n\n\n\n\n\n\nOur data contains samples in rows, and features in columns.\n\n\n\n\n\n\nWe will start with doing statistics (one factor) and we have a peak intensity table, so the file called lcms_table.csv is a good one for us to look at.\n\n\n\nLCMS sample data\n\n\nIn this data, we have features in rows and samples in columns. There is also a header row which contains the unique sample names, and a second row called Label which contains the groups. We will need to adjust our data to look like this. This includes\n\nAdding a row called Label which contains the sample groups (here, QC, HATS, LA2213, or OH8243).\n\n\n\n\nAdd a row which contains Label and the tomato identities\n\n\n\nCreating a single column with the mz_rt as a unique identifier. We can do this with the function =CONCATENATE() in Excel. Then we can fill down the row. Then be sure to copy this column and Paste &gt; Paste special &gt; Values since when we later remove some of the other columns that are inputs to this one, the formulas will break.\n\n\n\n\nHow to combine the row m/z and row retention time columns to be one unique mz_rt\n\n\n\nNow we can remove row ID, row m/z and row retention time.\n\nOur data should now look like this:\n\n\n\nOur data ready to import into MetaboAnalyst"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#data-check",
    "href": "activities/04_metaboanalyst.html#data-check",
    "title": "Data analysis with MetaboAnalyst",
    "section": "3.1 Data check",
    "text": "3.1 Data check\nOnce the data has been imported, we can first look at the data integrity check.\n\n\n\nData integrity check\n\n\nWe see that:\n\nSamples are in columns and features in rows\nOur data is in .csv format\nWe have 42 samples and 2514 peaks - this is what we would expect\nSamples are not paired - this is what we would expect\n3 groups were dtected - I might expect 4 groups - OH8243, LA2213, HATS, and QCs. If we click the ‚ÄúEdit Groups‚Äù button we can see that the Label for each of our groups has inherited correctly, but that it seems like QC isn‚Äôt being counted as a group.\nA total of 0 (0%) missing values were detected - this makes sense as data that is missing here is coded as zero.\n\nIf we agree with this, we can click Proceed. MetaboAnalyst will bring you directly to the Data filtering step, but we can go back one to see how missing values are handled."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#missing-values",
    "href": "activities/04_metaboanalyst.html#missing-values",
    "title": "Data analysis with MetaboAnalyst",
    "section": "3.2 Missing values",
    "text": "3.2 Missing values\nHere you can tell MetaboAnalyst what you want done with missing values. Here we don‚Äôt have any, but if you did you could indicate:\n\nWhether you want to remove features that have a lot of missing values\nWhat you want to do to estimate the remaining missing values."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#data-filters",
    "href": "activities/04_metaboanalyst.html#data-filters",
    "title": "Data analysis with MetaboAnalyst",
    "section": "3.3 Data filters",
    "text": "3.3 Data filters\nIn this step, we can filter variables based on different rules. These include:\n\nFeatures that are very variable i nthe QCs. We don‚Äôt need to do this because we already did it manually.\nFeatures that are near constant across the conditions (i.e., those that have very low standard deviations)\nFeatures that are very low\n\nI prefer to do this kind of filtering outside of MetaboAnalyst - you could do this in Excel if you wanted to. Here, we are not going to do any more filtering. We can just click Proceed (and skip this filtering)."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#pca-with-our-qcs",
    "href": "activities/04_metaboanalyst.html#pca-with-our-qcs",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.1 PCA with our QCs",
    "text": "5.1 PCA with our QCs\nWhen we look at the PCA tab, we can see:\n\nAn overview - which includes PCA scores plot for each combination of PCs 1 through 5. The most conventional one to look at would be PC1 vs PC2.\nA scree plot - this shows us how much variation is explained by each of the PCs\nA 2D scores plot - this shows us each sample‚Äôs new coordinates in a reduced dimensionality space. We can indicate what PC should be on which axis (though again it‚Äôs hard for me to think of a reason to do something different from PC1 on x and PC2 on y)\nA loadings plot - this shows us how the variables (here, features) are weighted in contributing to each PC\n3D plots - I find these to be terrible and never use them\nA biplot - combines the scores and the loadings, and can get very overplotted for MS metabolomics data.\n\nLet‚Äôs look at each plot, talk about what it shows us, and try to interpret the data in the context of this particular experiment.\n\n\n\nA 5 component PCA scores plot\n\n\nIn the PCA overview, we can see a big chunk of variation (63%) is explained on PC1. We also see that our groups are really clearly separated when viewing PC1 vs.¬†PC2.\n\n\n\nA scree plot\n\n\nWhen looking at the scree plot, we again see a large percentage of variation explained by PC1, and less variation explained by each subsequent PC. This is going to always be the case - PC2 always explains more variation than PC3, and PC3 more than PC4. But we can interpret this to be that drawing two PCs explains 78.4% of the metabolic variation in our dataset, and that adding additional PCs doesn‚Äôt explain all that much additional variation.\n\n\n\nA scores plot including our QCs\n\n\nWhen we think of PCAs, what we are typically thinking of is a scores plot. In this one, we see a few important things:\n\nOur QCs are clustering very closely together - this is a good piece of data to convince both ourselves and our readers that our data is of high quality. We are able to measure the same thing very reproducibly.\nPC1 mostly separates LA2213 (the wild tomato) from OH8243 and HATS (the cultivated tomatoes).\nPC2 mostly separates OH8243 (the commercial parent) from HATS (the tomato that has been introgressed for the trait of high steroidal alkaloids).\nOverall, HATS and OH8243 are more similar than either of those two are to LA2213.\n\nYou can adjust in MetaboAnalyst how this plot looks. I tend to turn sample names and confidence intervals off because I think they crowd the plot. You can click on the little paint palette to be able to specify how your plot looks (change colors and shapes) and download a high quality image.\nI would always put a figure like this in the supplementary materials of my papers so readers can get confidence that our data is worth spending time interpreting.\n\n\n\nA PCA loadings plot, and in this case a big mess\n\n\nThe loadings plot helps us to interpret that is driving separation on PC1 and PC2. Interpreting this alongside our scores plot, in this case, points on the far right of the plot should be higher in LA2213 and points on the far left should be higher oh OH8243 and HATS. Points that are close to the top of the plot should be higher in HATS and those towards the bottom higher in OH8243 or LA2213.\nFor example, if we click on the right most point on the loadings plot, we can see that this represents the feature with the identifier 578.5296_5.0895. We can select a boxplot and see the relative intensity of this feature across our sample groups.\n\n\n\nGroup relative intensities of 578.5296_5.0895\n\n\nSimilarly, the point with the highest value on the y axis 474.3578_6.5878 is highest in HATS and lower in the other tomatoes.\n\n\n\nGroup relative intensities of 474.3578_6.5878\n\n\nYou can also download the raw data for your loadings. This might be helpful to see which features have the highest and lowest values on loadings 1 and 2. You can also view boxplots of the feature abundances in this same view and sort your loadings."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#pca-without-qcs",
    "href": "activities/04_metaboanalyst.html#pca-without-qcs",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.2 PCA without QCs",
    "text": "5.2 PCA without QCs\nThat that we can see that our QCs are tightly clustering together, we can save that PCA scores plot and remove our QC samples. We can do that by navigating to Processing &gt; Data editor in the left part of the browser.\n\n\n\nRemoving the QCs from our data\n\n\nDon‚Äôt forget to re-normalize.\nThis dataset (without the QCs) is what I would use for the rest of this analysis.\nHere is a PCA that could be used as a figure in a paper.\n\n\n\nPCA scores plot without the QCs\n\n\nWe see the message of this PCA looks basically the same as when we had our QCs. But, we wouldn‚Äôt want to progress to univariate or multivariate statistics including our QCs as a group, since it is not an experimental group per se. I‚Äôm not going to go through all the plots again since we just did that with the QCs and the message is not changing."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#anova",
    "href": "activities/04_metaboanalyst.html#anova",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.3 ANOVA",
    "text": "5.3 ANOVA\nWe might be interested to see which features are significantly different across the 3 tomato groups, and we can assess that using ANOVA. We can set what our FDR cut-off should be, and I am going to be a little bit more stringent here and set this to 0.01.\nIn this case, this step isn‚Äôt that helpful since 2168 out of 2514 features are significantly different between at least two of our tomato groups. That doesn‚Äôt help us narrow down much of anything.\n\n\n\nANOVA visualization of features significantly different across tomato groups"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#dendrogram-i.e.-hierarchical-clustering",
    "href": "activities/04_metaboanalyst.html#dendrogram-i.e.-hierarchical-clustering",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.4 Dendrogram (i.e., hierarchical clustering)",
    "text": "5.4 Dendrogram (i.e., hierarchical clustering)\nHierarchical clustering is a nice unsupervised way to see how the metabolomic profile of your samples compare. Many people when talking about their PCA scores plot refer to ‚Äúnatural clusters‚Äù in their data. The problem with this is that PCA is not a clustering algorithm. If you want to show heuristically natural clusters in your data, hierarchical clustering can help\n\n\n\nHierarchical clustering of samples\n\n\nHere we can see a dendrogram (like a phylogenetic tree) which clusters samples based on a distance metric and clustering algorithm. Samples that are more similar should be close to one another, and the longer the distance is on the x-axis, the less similar samples are.\nIn this particular case, we can see 3 clusters derived of our 3 tomato groups. We can also see that HATS and OH8243 are more closely clustered compared to LA2213. This is consistent with what we see in our PCA."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#k-means-a-different-type-of-clustering",
    "href": "activities/04_metaboanalyst.html#k-means-a-different-type-of-clustering",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.5 K-means (a different type of clustering)",
    "text": "5.5 K-means (a different type of clustering)\nK-means is another unsupervised clustering technique that has a similarly interpreted output to HCA but with different underlying assumptions and math. Here, we have to provide how much clusters we have (you can learn more about how to figure this out here. I am going to pick 3 because we have 3 tomato groups, and want to know if our samples will segregate based on group identity across 3 clusters.\n\n\n\nK-means clustering with 3 clusters\n\n\nHere, we can see that when we indicate 3 clusters, the tomato groups separate perfectly. This gives us some additional support that we have 3 distinct clusters in our data, and those are our tomato groups.\nAlternatively, if we look for 2 or 4 clusters, we will see something different.\n\n\n\nK-means clustering with 2 clusters\n\n\n\n\n\nK-means clustering with 4 clusters\n\n\nThis looks to me actually like 3 clusters, but it‚Äôs not. If we click the spreadsheet beside the artists palette, we can see which samples belong to which cluster.\n\n\n\nK-means cluster identities with 4 clusters"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#heatmaps",
    "href": "activities/04_metaboanalyst.html#heatmaps",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.6 Heatmaps",
    "text": "5.6 Heatmaps\nA heatmap is a nice way to look at lots of data at once. It doesn‚Äôt give you lots of resolution but allows you to see trends in your data you can then investigate in other ways.\nWhen you create the heatmap, you can elect to:\n\nData source: use normalized or the original data - I‚Äôd use normalized\nStandardization: I would autoscale features - this way each feature is normalized so more abundant ones aren‚Äôt weighted more heavily\nDistance measure and clustering method: these are for the hierarchical clustering\n\n\n\n\nA heatmap where both smaples and features are clustered\n\n\nAs we saw with the HCA, our samples are clustering by which tomato group they are a part of. But here we can see which features have the same trends and this can be useful to us!"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#correlations",
    "href": "activities/04_metaboanalyst.html#correlations",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.7 Correlations",
    "text": "5.7 Correlations\nThe correlations analysis calculates a correlation coefficient between every pair of features. This plot is really crowed and hard to look at, but the concept is useful. We might be interested to know which groups of features have the same pattern of response across our samples. Or to know which features are highly negatively correlated.\nI find this format hard to sort through, but this approach to be generally useful.\n\n\n\nA correlation heatmap clustering both features and samples"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#random-forest",
    "href": "activities/04_metaboanalyst.html#random-forest",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.8 Random forest",
    "text": "5.8 Random forest\nThis isn‚Äôt that good of an example for random forest classification since our samples are so different.\nRandom forest is a machine learning class predition approach. In MetaboAnalyst, you can pick how many trees to draw and how many predictors to use.\n\n\n\nOutput of random forest classification\n\n\nIn this case, we can see that our out of bag error rate is 0, and with using very few trees, we are able to perfectly classify samples as to whether they belong to each of the three tomato groups.\nIf you look under the variable importance tab you can see the features that are most important in generating this model. You can look at this as a figure, or you can download a file to see how each one affects the mean decrease in accuracy. You can sort this list from high to low or low to high.\n\n\n\nVariable importance predictors to our random forest model\n\n\nYou can then look for each of those figures how the relative intensity compares between tomato groups.\n\n\n\nIntensity of 1035.29429_5.1117 across our three tomato groups\n\n\nAnother option for random forest is to have a training and a test set. As far as I can tell, MetaboAnalyst does this by calculating the out of bag error but there are also other approaches."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#pattern-hunter",
    "href": "activities/04_metaboanalyst.html#pattern-hunter",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.9 Pattern hunter",
    "text": "5.9 Pattern hunter\nYou might be interested in features that follow a certain pattern across your samples. Say you have samples that are processed for different amounts of time, you may be interested in knowing which features follow that trend (i.e., are lower or higher across processing). The pattern hunter helps us find features like this.\nIn this case, I am interested in features that follow this trend: OH8243 &lt; HATS &lt; LA2213.\n\n\n\nLooking for patterns - here features that are highest in LA2213, second highest in HATS, and lowest in OH8243.\n\n\nYou can also click on the spreadsheet to see the correlation among each feature, and then view what the relationship looks like.\n\n\n\nRelative intensity of 1081.56453_4.64531 across tomato groups"
  },
  {
    "objectID": "activities/04_metaboanalyst.html#fold-change-analysis",
    "href": "activities/04_metaboanalyst.html#fold-change-analysis",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.10 Fold change analysis",
    "text": "5.10 Fold change analysis\nThere are some analyses that can be only done with two groups (versus the three that we‚Äôve had for the previous analyses). To demonstrate how these work, I am going to only keep the samples that are a part of the HATS and OH8243 groups.\nI can edit that using the Data editor on the leftside panel.\n\n\n\nData editor to remove samples or gorups\n\n\nRemember that when you do this, you will need to re-normalized.\nNow, we have access to new analyses. We will start with fold change analysis.\nWe can set what our fold change threshold is going to be, and what the direction of the comparison is.\n\n\n\nFold change analysis between OH8243 and HATS\n\n\nWe can then click on any of the points to see how the relative intensity of that features is different among our groups.\n\n\n\nRelative intensity of 474.3577_6.5878 between our two tomato groups\n\n\nYou can also click on the spreadsheet icon to view and/or download the full set of mean fold changes for each feature between the groups."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#t-test",
    "href": "activities/04_metaboanalyst.html#t-test",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.11 T-test",
    "text": "5.11 T-test\nLike we did with an ANOVA for &gt; 2 groups, we might want to see if there are any differences in the intensity of each feature between our two tomato groups. We can do that using t-tests (parametric) or Wilcoxon rank-sum tests. We can select whether we have equal variance between or groups, whether we want to use a non-parametric test (if data is non-normal), what our p-value threshold is, and whether we are doing a multiple testing correct (you should be!).\n\n\n\nA visualization of a FDR corrected t-test between OH8243 and HATS\n\n\nYou again can click on the spreadhseet icon to download the full feature set along with p-values, FDR adjusted p-values, and -log10 p-values."
  },
  {
    "objectID": "activities/04_metaboanalyst.html#volcano-plots",
    "href": "activities/04_metaboanalyst.html#volcano-plots",
    "title": "Data analysis with MetaboAnalyst",
    "section": "5.12 Volcano plots",
    "text": "5.12 Volcano plots\nVolcano plots are nice as they incorporate both those features that are significantly different between groups, and the fold chance of that differences.\n\n\n\nA volcano plot incorporating both significance and fold change\n\n\nIf you click on any feature, you can see the relative intensity across our two groups."
  },
  {
    "objectID": "activities/02_design-experiment.html",
    "href": "activities/02_design-experiment.html",
    "title": "Design your own experiment",
    "section": "",
    "text": "Get together in small groups and spend ~10-15 min designing a metabolomics experiment around a research question of one (or more than one) of the group members. After this discussion period, we will reconvene in our larger group and each team will share the following information about your experiments:\n\nWhat was your research question?\nWhat is your hypothesis?\nWhat samples are you selecting to analyze?\nHow will they be collected?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/04_data-analysis.html#now-that-we-have-our-feature-table-what-happens-next",
    "href": "lectures/04_data-analysis.html#now-that-we-have-our-feature-table-what-happens-next",
    "title": "Analysis of metabolomics data",
    "section": "Now that we have our feature table, what happens next?",
    "text": "Now that we have our feature table, what happens next?\n\nData normalization/transformation/scaling\nAssess for quality (and make adjustments if needed)\nUnivariate analysis (supervised)\nMultivariate analysis (supervised or unsupervised)"
  },
  {
    "objectID": "lectures/04_data-analysis.html#data-pre-treatment",
    "href": "lectures/04_data-analysis.html#data-pre-treatment",
    "title": "Analysis of metabolomics data",
    "section": "Data pre-treatment",
    "text": "Data pre-treatment\n\nDifferent types of data centering, scaling, and transformation, from van den Berg et al., Genomics 2006"
  },
  {
    "objectID": "lectures/04_data-analysis.html#centering",
    "href": "lectures/04_data-analysis.html#centering",
    "title": "Analysis of metabolomics data",
    "section": "Centering",
    "text": "Centering\n\n\nCentering converts all relative intensities to be centered around zero instead of around mean concentrations\nThis is particularly useful in metabolomics given that more intense features are not necessarily more abundant\nI like to center when conducting principal components analysis (PCA, more on this later)\n\n\n\n\n\n\nPCA not centered\n\n\n\n\n\n\nPCA centered"
  },
  {
    "objectID": "lectures/04_data-analysis.html#scaling",
    "href": "lectures/04_data-analysis.html#scaling",
    "title": "Analysis of metabolomics data",
    "section": "Scaling",
    "text": "Scaling\n\nUses data dispersion as a part of a scaling factor\n\nAuto scaling: mean = 0, standard deviation = 1 (for each feature)\nPareto scaling: like auto but scaled to square root of the st dev. The most common scaling used for metabolomics\nIn general I find this not necessary and will just log transform (more coming next)"
  },
  {
    "objectID": "lectures/04_data-analysis.html#log-transformation---tables",
    "href": "lectures/04_data-analysis.html#log-transformation---tables",
    "title": "Analysis of metabolomics data",
    "section": "Log transformation - tables",
    "text": "Log transformation - tables\n\nReduce heteroscedasicity\nMakes large values proportionally smaller than small ones\nMakes distributions more normal\n\n\n\n\n\n\n\n\nsample\nfeature_1\nfeature_2\n\n\n\n\ncontrol\n10000\n1000\n\n\ntreatment\n11000\n2000\n\n\n\n\n\n\n\n\n\n\n\nsample\nlog2_feature_1\nlog2_feature_2\n\n\n\n\ncontrol\n13.29\n9.97\n\n\ntreatment\n13.43\n10.97"
  },
  {
    "objectID": "lectures/04_data-analysis.html#log-transformation---plots",
    "href": "lectures/04_data-analysis.html#log-transformation---plots",
    "title": "Analysis of metabolomics data",
    "section": "Log transformation - plots",
    "text": "Log transformation - plots\n\nLog transforming can make your data look more normally distributed"
  },
  {
    "objectID": "lectures/04_data-analysis.html#sample-wise-normalization",
    "href": "lectures/04_data-analysis.html#sample-wise-normalization",
    "title": "Analysis of metabolomics data",
    "section": "Sample wise normalization",
    "text": "Sample wise normalization\n\nAdjust based on weight or volume (very common)\nAdjust based on intensity of an internal standard (used sometimes)\nAdjust based on total signal (used in older MS papers and for NMR but not really in MS)"
  },
  {
    "objectID": "lectures/04_data-analysis.html#assessing-data-quality",
    "href": "lectures/04_data-analysis.html#assessing-data-quality",
    "title": "Analysis of metabolomics data",
    "section": "Assessing data quality",
    "text": "Assessing data quality\n\nHow can we go about assessing if our data is of high quality when we have thousands of features and don‚Äôt know what to expect?"
  },
  {
    "objectID": "lectures/04_data-analysis.html#overlay-your-bpcs",
    "href": "lectures/04_data-analysis.html#overlay-your-bpcs",
    "title": "Analysis of metabolomics data",
    "section": "Overlay your BPCs",
    "text": "Overlay your BPCs\n\nOverlaid base peak chromatograms in MZmine"
  },
  {
    "objectID": "lectures/04_data-analysis.html#zoom-in-and-look-at-your-bpcs-carefully",
    "href": "lectures/04_data-analysis.html#zoom-in-and-look-at-your-bpcs-carefully",
    "title": "Analysis of metabolomics data",
    "section": "Zoom in and look at your BPCs carefully",
    "text": "Zoom in and look at your BPCs carefully\n\nOverlaid base peak chromatograms in MassHunter"
  },
  {
    "objectID": "lectures/04_data-analysis.html#look-at-a-single-feature-for-retention-time-shifting",
    "href": "lectures/04_data-analysis.html#look-at-a-single-feature-for-retention-time-shifting",
    "title": "Analysis of metabolomics data",
    "section": "Look at a single feature for retention time shifting",
    "text": "Look at a single feature for retention time shifting\n\nOverlaid extracted ion chromatograms in MassHunter"
  },
  {
    "objectID": "lectures/04_data-analysis.html#create-boxplots-of-features-by-sample",
    "href": "lectures/04_data-analysis.html#create-boxplots-of-features-by-sample",
    "title": "Analysis of metabolomics data",
    "section": "Create boxplots of features by sample",
    "text": "Create boxplots of features by sample\n\n\nOrdering by sample groups \n\nOrdering by run order"
  },
  {
    "objectID": "lectures/04_data-analysis.html#your-qcs-should-cluster-in-a-pca",
    "href": "lectures/04_data-analysis.html#your-qcs-should-cluster-in-a-pca",
    "title": "Analysis of metabolomics data",
    "section": "Your QCs should cluster in a PCA",
    "text": "Your QCs should cluster in a PCA\n\nA PCA where the QCs cluster beautifully"
  },
  {
    "objectID": "lectures/04_data-analysis.html#data-analysis-can-be-unsupervised-or-supervised",
    "href": "lectures/04_data-analysis.html#data-analysis-can-be-unsupervised-or-supervised",
    "title": "Analysis of metabolomics data",
    "section": "Data analysis can be unsupervised or supervised",
    "text": "Data analysis can be unsupervised or supervised\n\n\nUnsupervised (model does not know which samples belong to which groups):\n\nPrincipal components analysis (PCA)\nHierarchical clustering analysis (HCA)\nK-means clustering\n\n\nSupervised (model knows group composition):\n\nUnivariate (or multivariate) group comparisons\nPredictive/classification models\n\nPartial least squares regression (PLS-R)\nPLS-discriminant analysis (PLS-DA)\nRandom forest\nArtificial neural networks\nOthers"
  },
  {
    "objectID": "lectures/04_data-analysis.html#unsupervised-principal-components-analysis-pca",
    "href": "lectures/04_data-analysis.html#unsupervised-principal-components-analysis-pca",
    "title": "Analysis of metabolomics data",
    "section": "Unsupervised: Principal Components Analysis (PCA)",
    "text": "Unsupervised: Principal Components Analysis (PCA)\n\n\n\nA dimensionality reduction approach that transforms our data into a new system where principal coordinates (PCs) are drawn maximizing variation in our data.\nEach new PC is orthogonal to the previous one.\nCan interpret points closer together as more similar that those further apart\nThe loadings plot helps us understand which features are most influential for each PC\n\n\n\n\n\nFig from https://www.ibm.com/topics/principal-component-analysis"
  },
  {
    "objectID": "lectures/04_data-analysis.html#interpreting-pcas",
    "href": "lectures/04_data-analysis.html#interpreting-pcas",
    "title": "Analysis of metabolomics data",
    "section": "Interpreting PCAs",
    "text": "Interpreting PCAs\n\nPCA A) scores and B) loadings (from Dzakovich et al., 2022)"
  },
  {
    "objectID": "lectures/04_data-analysis.html#what-you-should-be-doing-with-your-pcas",
    "href": "lectures/04_data-analysis.html#what-you-should-be-doing-with-your-pcas",
    "title": "Analysis of metabolomics data",
    "section": "What you should be doing with your PCAs",
    "text": "What you should be doing with your PCAs\n\nCheck for samples that are very different from the rest\nShow that your QCs cluster together\nLook for batch effects\nObserve the distance between your samples within a group, and between groups\nSee how much variation is being explained by each PC\nLook for clusters in your data"
  },
  {
    "objectID": "lectures/04_data-analysis.html#unsupervised-hierarchical-clustering-analysis-hca",
    "href": "lectures/04_data-analysis.html#unsupervised-hierarchical-clustering-analysis-hca",
    "title": "Analysis of metabolomics data",
    "section": "Unsupervised: Hierarchical Clustering Analysis (HCA)",
    "text": "Unsupervised: Hierarchical Clustering Analysis (HCA)\n\n\nAn approach to ‚Äúcluster‚Äù samples based on their distance/similarity\nFind two most similar samples and merge into cluster, and repeat\nDifferent methods for determining both distance and linkage\nLike a phylogenic tree - samples that are closer together are more similar\nCan cluster samples, features, or both\nStatQuest video on hierarchical clustering"
  },
  {
    "objectID": "lectures/04_data-analysis.html#hca-applied-to-metabolomics",
    "href": "lectures/04_data-analysis.html#hca-applied-to-metabolomics",
    "title": "Analysis of metabolomics data",
    "section": "HCA applied to metabolomics",
    "text": "HCA applied to metabolomics\n\nA hierarchical clustering example from Dzakovich et al., 2024"
  },
  {
    "objectID": "lectures/04_data-analysis.html#unsupervised-k-means-clustering",
    "href": "lectures/04_data-analysis.html#unsupervised-k-means-clustering",
    "title": "Analysis of metabolomics data",
    "section": "Unsupervised: K-means clustering",
    "text": "Unsupervised: K-means clustering\n\n\n\nRandomly assigns all samples to be a part of \\(k\\) clusters\nCalculates centroids and assigns samples to the nearest one\nIterates until centroids stability or reach some maximum\nCan figure out number of clusters using a scree plot to visualize within cluster sums\nStatQuest explains K-means clustering\n\n\n\n\n\nFrom Wikipedia"
  },
  {
    "objectID": "lectures/04_data-analysis.html#supervised-univariate-testing-the-categories",
    "href": "lectures/04_data-analysis.html#supervised-univariate-testing-the-categories",
    "title": "Analysis of metabolomics data",
    "section": "Supervised: Univariate testing, the categories",
    "text": "Supervised: Univariate testing, the categories\n\nTwo group tests\n\nt-test (parametric)\nWilcoxon rank sum test (non-parameric)\n\nMore than two group tests\n\nANOVA (parametric)\nKruskal Wallis test (non-parametric)\nBoth followed by post-hoc means separation"
  },
  {
    "objectID": "lectures/04_data-analysis.html#big-p-little-n-the-curse-of-dimensionality",
    "href": "lectures/04_data-analysis.html#big-p-little-n-the-curse-of-dimensionality",
    "title": "Analysis of metabolomics data",
    "section": "Big p, little n: the curse of dimensionality",
    "text": "Big p, little n: the curse of dimensionality\n\nWith metabolomics, we almost always have more features (p) than samples (n)\nThis means we are making a lot of comparisons\nIf we want to know if each figure is differentially abundant across two groups, we can do a series of t-tests.\nIf we set Œ± = 0.05, then we have a 5% chance for rejecting the null when it is true (false positive, type I error)\nIf we do 1000 tests, and each one has a 5% of a false positive, then we would predict to have 50 false positives due to chance alone - features that have p &lt; 0.05 but are not truly different between our groups"
  },
  {
    "objectID": "lectures/04_data-analysis.html#ways-to-control-for-multiple-testing",
    "href": "lectures/04_data-analysis.html#ways-to-control-for-multiple-testing",
    "title": "Analysis of metabolomics data",
    "section": "Ways to control for multiple testing",
    "text": "Ways to control for multiple testing\n\nBonferroni correction: \\(Œ±/number\\,of\\,comparisons\\) (very conservative, leads to false negatives)\nBenjamini Hochberg false discovery rate correction: adjust overall error rate to be Œ± (tries to balance false negatives and positives)\n\n\n\n\n\nFig. from https://geneviatechnologies.com/blog/what-is-multiple-testing-correction/"
  },
  {
    "objectID": "lectures/04_data-analysis.html#should-i-use-parametric-or-non-parametric-tests",
    "href": "lectures/04_data-analysis.html#should-i-use-parametric-or-non-parametric-tests",
    "title": "Analysis of metabolomics data",
    "section": "Should I use parametric or non-parametric tests?",
    "text": "Should I use parametric or non-parametric tests?"
  },
  {
    "objectID": "lectures/04_data-analysis.html#multivariate-testing-in-metabolomics",
    "href": "lectures/04_data-analysis.html#multivariate-testing-in-metabolomics",
    "title": "Analysis of metabolomics data",
    "section": "Multivariate testing in metabolomics",
    "text": "Multivariate testing in metabolomics\n\nMultivariate approaches allow the investigation of more than one feature at once\nApproaches can be unsupervised or supervised\nBe sure to select the most appropriate method given the nature of your data and question"
  },
  {
    "objectID": "lectures/04_data-analysis.html#supervised-pls-da",
    "href": "lectures/04_data-analysis.html#supervised-pls-da",
    "title": "Analysis of metabolomics data",
    "section": "Supervised: PLS-DA",
    "text": "Supervised: PLS-DA\n\n\n\nPartial least squares discriminate analysis (PLS-DA) approaches optimize separation between groups\nTwo data matrices, X: contains your features, Y: contains group identity\nBe sure to look at R^2 and Q^2 for both training and test sets\nHas a tendency to overfit\nLearn more about PLS-DA in this manuscript by Brereton and Lloyd, J Chemometrics 2014\n\n\n\n\n\nRuiz-Perez et al., BMC Bioinformatics 2020"
  },
  {
    "objectID": "lectures/04_data-analysis.html#random-forest",
    "href": "lectures/04_data-analysis.html#random-forest",
    "title": "Analysis of metabolomics data",
    "section": "Random forest",
    "text": "Random forest"
  },
  {
    "objectID": "lectures/04_data-analysis.html#artificial-neural-networks",
    "href": "lectures/04_data-analysis.html#artificial-neural-networks",
    "title": "Analysis of metabolomics data",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\n\n\n\n¬© Jessica Cooperstone, 2024"
  },
  {
    "objectID": "lectures/05_compound-id.html#how-do-we-go-from-features-to-metabolites",
    "href": "lectures/05_compound-id.html#how-do-we-go-from-features-to-metabolites",
    "title": "Compound identification in metabolomics",
    "section": "How do we go from features to metabolites?",
    "text": "How do we go from features to metabolites?\n\nIndividually (one feature at a time)\n\nManual, very high confidence in ID\n\nIn bulk (many features at a time)\n\nComputational, generally lower confidence in ID"
  },
  {
    "objectID": "lectures/05_compound-id.html#starting-with-a-mz",
    "href": "lectures/05_compound-id.html#starting-with-a-mz",
    "title": "Compound identification in metabolomics",
    "section": "Starting with a m/z",
    "text": "Starting with a m/z\n\n\n\nA feature of interest differentially present in commercial vs.¬†wild tomatoes. We want to know what this is.\nUHPLC-QTOF-MS, ESI+, reversed phase C18, Methanol extract\nA useful adduct calculator\n\n\n\n\n\nRelative intensity of 1092.5590 in tomatoes m/z in tomatoes"
  },
  {
    "objectID": "lectures/05_compound-id.html#search-in-ms-databases---hmdb",
    "href": "lectures/05_compound-id.html#search-in-ms-databases---hmdb",
    "title": "Compound identification in metabolomics",
    "section": "Search in MS databases - HMDB",
    "text": "Search in MS databases - HMDB\nSearch &gt; LC-MS Search\n\n\n\n\n\nScreenshot of an HMDB MS1 search\n\n\n\n\nEnter your mass and mode\nIndicate adduct type (can put unknown if you don‚Äôt know)\nSelect a mass error (5 ppm is good for a QTOF)\nSearch"
  },
  {
    "objectID": "lectures/05_compound-id.html#evaluate-search-results",
    "href": "lectures/05_compound-id.html#evaluate-search-results",
    "title": "Compound identification in metabolomics",
    "section": "Evaluate search results",
    "text": "Evaluate search results\nThink about:\n\nWhich structures are plausible?\nMake sense in your biological system\nMake sense in your extraction\nMake sense based on retention time\nWhich adducts are more likely?\nThe top result is not necessarily your ID!"
  },
  {
    "objectID": "lectures/05_compound-id.html#access-or-collection-msms-fragmentation-data",
    "href": "lectures/05_compound-id.html#access-or-collection-msms-fragmentation-data",
    "title": "Compound identification in metabolomics",
    "section": "Access or collection MS/MS fragmentation data",
    "text": "Access or collection MS/MS fragmentation data\n\nMS/MS spectra of 1092.5590 at 65eV in +ESI"
  },
  {
    "objectID": "lectures/05_compound-id.html#download-structure",
    "href": "lectures/05_compound-id.html#download-structure",
    "title": "Compound identification in metabolomics",
    "section": "Download structure",
    "text": "Download structure\nhttp://www.hmdb.ca/spectra/ms_ms/59530\n\nDownload an .sdf file of your structure"
  },
  {
    "objectID": "lectures/05_compound-id.html#import-into-chemsketch",
    "href": "lectures/05_compound-id.html#import-into-chemsketch",
    "title": "Compound identification in metabolomics",
    "section": "Import into ChemSketch",
    "text": "Import into ChemSketch\n\nImport your .sdf file into ChemSketch"
  },
  {
    "objectID": "lectures/05_compound-id.html#set-chemsketch-to-provide-you-structural-information",
    "href": "lectures/05_compound-id.html#set-chemsketch-to-provide-you-structural-information",
    "title": "Compound identification in metabolomics",
    "section": "Set ChemSketch to provide you structural information",
    "text": "Set ChemSketch to provide you structural information\n\n\nSelect which parameters you want printed: Tools &gt; Calculate &gt; Select properties to calculate &gt; Select ‚ÄúMolecular Formula‚Äù, ‚ÄúMonoisotopic Mass‚Äù, ‚Äú[M+H]+‚Äù\nHave ChemSketch calculate those parameters: Tools &gt; Calculate &gt; Selected properties &gt; Copy to editor.\n\n\n\n\n\n\nScreenshot of acetoxytomatine in ChemSketch with parameters calculated"
  },
  {
    "objectID": "lectures/05_compound-id.html#breaking-bonds-to-rationalize-fragments",
    "href": "lectures/05_compound-id.html#breaking-bonds-to-rationalize-fragments",
    "title": "Compound identification in metabolomics",
    "section": "Breaking bonds to rationalize fragments",
    "text": "Breaking bonds to rationalize fragments\n\n\n\nUse the eraser (delete) to break bonds (learn more here re: using ChemSketch)\nSelect a bond and it will be deleted\nThe details will stay with the larger fragment. Highlight the smaller piece to recalculate parameters.\nAccount for any rearrangement or additions/subtractions\n\n\n\n\n\nClick on the delete eraser"
  },
  {
    "objectID": "lectures/05_compound-id.html#breaking-bonds-to-rationalize-fragments-1",
    "href": "lectures/05_compound-id.html#breaking-bonds-to-rationalize-fragments-1",
    "title": "Compound identification in metabolomics",
    "section": "Breaking bonds to rationalize fragments",
    "text": "Breaking bonds to rationalize fragments\n\nCleaving at the acetoxy group"
  },
  {
    "objectID": "lectures/05_compound-id.html#breaking-bonds-to-rationalize-fragments-2",
    "href": "lectures/05_compound-id.html#breaking-bonds-to-rationalize-fragments-2",
    "title": "Compound identification in metabolomics",
    "section": "Breaking bonds to rationalize fragments",
    "text": "Breaking bonds to rationalize fragments\n\nCleaving at the acetoxy group"
  },
  {
    "objectID": "lectures/05_compound-id.html#how-can-we-improve-the-confidence-of-our-id",
    "href": "lectures/05_compound-id.html#how-can-we-improve-the-confidence-of-our-id",
    "title": "Compound identification in metabolomics",
    "section": "How can we improve the confidence of our ID?",
    "text": "How can we improve the confidence of our ID?\n\nReference against publicly available MS/MS spectra\nPurchase authentic standard and compare\nSynthesize standard, confirm by NMR, and hope you‚Äôre right ü•π\nCompare to MS/MS spectra of similar compounds\nCompare to a sample that you know has your compound of interest"
  },
  {
    "objectID": "lectures/05_compound-id.html#msms-online-databases",
    "href": "lectures/05_compound-id.html#msms-online-databases",
    "title": "Compound identification in metabolomics",
    "section": "MS/MS online databases",
    "text": "MS/MS online databases\n\nExperimental spectra\nPredicted spectra"
  },
  {
    "objectID": "lectures/05_compound-id.html#msms-predicted-spectra-of-acetoxytomatine",
    "href": "lectures/05_compound-id.html#msms-predicted-spectra-of-acetoxytomatine",
    "title": "Compound identification in metabolomics",
    "section": "MS/MS predicted spectra of acetoxytomatine",
    "text": "MS/MS predicted spectra of acetoxytomatine\nhttp://www.hmdb.ca/spectra/ms_ms/59530\n\nPredicted spectra for (23R)-acetoxytomatine"
  },
  {
    "objectID": "lectures/05_compound-id.html#msms-spectral-databases",
    "href": "lectures/05_compound-id.html#msms-spectral-databases",
    "title": "Compound identification in metabolomics",
    "section": "MS/MS spectral databases",
    "text": "MS/MS spectral databases\n\nGNPS\nMassBank of North America (MONA)\nHMDB\nOthers that you have to pay for (METLIN, NIST, mzCloud)\nA review on spectral libraries by Bittremieux et al., Metabolomics 2022."
  },
  {
    "objectID": "lectures/05_compound-id.html#lc-ms-metabolite-id-workflow",
    "href": "lectures/05_compound-id.html#lc-ms-metabolite-id-workflow",
    "title": "Compound identification in metabolomics",
    "section": "LC-MS metabolite ID workflow",
    "text": "LC-MS metabolite ID workflow\n\nFrom Fitzgerald et al., ACS Omega 2017"
  },
  {
    "objectID": "lectures/05_compound-id.html#do-you-need-to-have-an-id-to-quantify",
    "href": "lectures/05_compound-id.html#do-you-need-to-have-an-id-to-quantify",
    "title": "Compound identification in metabolomics",
    "section": "Do you need to have an ID to quantify?",
    "text": "Do you need to have an ID to quantify?\n\n\n\n¬© Jessica Cooperstone, 2024"
  }
]